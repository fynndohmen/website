[
{
  "title": "Timing & Logging with Python Decorators: A Tiny Tool That Scales to Machine Learning",
  "content": [
    "This small script is a practical example of why Python decorators are so powerful: you can wrap existing functions without changing their internal code, and automatically add timing and logging behavior to every call.",
    "I built two reusable decorators (timeit and log_calls). They are simple, but they mirror exactly the kind of instrumentation you want in real ML workflows: measuring slow preprocessing steps, tracking training iterations, and keeping debugging output consistent across a codebase."
  ],
  "images": [],
  "description": [
    "### What this script is",
    "The code defines two decorators and uses them to wrap arbitrary functions:",
    "- `timeit`: measures and prints execution time using `time.perf_counter()`",
    "- `log_calls`: prints a small call trace like `[CALL] train_model()`",
    "",
    "Both decorators return a new function called `wrapper` that calls the original function (`func`) and adds extra behavior around it.",
    "",
    {
      "type": "image",
      "src": "machinelearning_pics/decorators_timeit_logcalls.png",
      "alt": "Python code showing two decorators: timeit and log_calls, using TypeVar T, Callable[..., T], and functools.wraps"
    },
    "",
    "---",
    "### How the decorators work (step by step)",
    "A decorator is just a function that takes another function and returns a new function.",
    "In this script, `timeit` works like this:",
    "- It receives `func` (the original function).",
    "- It defines `wrapper(*args, **kwargs)` so it can accept any arguments.",
    "- It records timestamps around the call.",
    "- It prints the elapsed time.",
    "- It returns the original result so the decorated function behaves the same as before.",
    "",
    "That last point matters: `result = func(*args, **kwargs)` and `return result` ensure that decorating does not break the function’s API. If the original function returns something useful (a model, a loss value, a DataFrame, etc.), the wrapper passes it through unchanged.",
    "",
    "`log_calls` is similar, but instead of timing it prints a message right before the function runs.",
    "",
    "---",
    "### Why `@wraps(func)` matters",
    "Both decorators use `@wraps(func)` from `functools`. Without it, the decorated function would lose helpful metadata and appear as `wrapper` everywhere.",
    "`@wraps` preserves important attributes such as:",
    "- function name (`__name__`)",
    "- docstring (`__doc__`)",
    "- annotations (`__annotations__`)",
    "- and a `__wrapped__` link used by inspection tools",
    "",
    "This is useful because debuggers, documentation generators, and tooling that inspects functions behave much more predictably when metadata is preserved.",
    "",
    "---",
    "### Why decorators are useful in Machine Learning",
    "Machine learning code quickly turns into a pipeline: data loading, cleaning, feature engineering, batching, training, evaluation, saving, and serving. Decorators are a clean way to add cross-cutting functionality to these steps without cluttering each function.",
    "",
    "**Common ML uses where decorators shine:**",
    "- **Profiling preprocessing**: identify slow steps in data cleaning or feature computation.",
    "- **Training instrumentation**: time epochs/steps and print structured logs.",
    "- **Caching expensive work**: reuse results for feature extraction or embedding generation.",
    "- **Retry and robustness**: wrap flaky IO (downloads, remote storage reads) with retry logic.",
    "- **Experiment tracking hooks**: automatically log parameters and metrics (MLflow/W&B style).",
    "- **Guardrails**: validate shapes and dtypes before a model forward pass to fail early.",
    "",
    "The big advantage is consistency: once you have a good decorator, you can apply it everywhere and keep your pipeline readable and maintainable.",
    "",
    "---",
    "### Key takeaway",
    "This script is small, but it demonstrates a pattern that scales: decorators let you add reusable infrastructure around ML code (timing, logging, caching, validation) while keeping the model and data logic focused and uncluttered."
  ],
  "date": "2026-02-15"
},
  {
  "title": "Streaming CSV Rows with a Generator: My First `read_rows()` Data Pipeline Building Block",
  "content": [
    "This tiny function looks almost too simple, but it taught me one of the most practical lessons about generators: you can process huge datasets row-by-row without loading everything into memory.",
    "`read_rows()` is a generator that reads a CSV file lazily and yields one dictionary per row. That single idea (streaming instead of materializing) shows up everywhere in machine learning—data preprocessing, batching, training loops, and model serving."
  ],
  "images": [],
  "description": [
    "### The function",
    {
      "type": "image",
      "src": "machinelearning_pics/read_rows_generator.png",
      "alt": "Python generator function read_rows() using csv.DictReader and yield to stream CSV rows"
    },
    "",
    "---",
    "### What I learned about generators from this",
    "**1) A generator does not compute all results upfront**",
    "Because the function contains `yield`, calling `read_rows(path)` does *not* read the whole CSV. It returns a generator object, and the file is only read when someone starts iterating over it.",
    "",
    "**2) `yield` pauses execution and resumes later**",
    "Each time the loop hits `yield row`, the function pauses and hands the current row to the caller. On the next iteration, execution continues exactly after that `yield` line, reading the next CSV row.",
    "",
    "**3) It is naturally one-pass (streaming)**",
    "The generator reads the file forward, row by row. Once the iterator reaches the end, it’s done. No list of all rows is created unless I explicitly build one (like `list(read_rows(...))`).",
    "",
    "**4) The return type tells the story**",
    "`Iterator[Dict[str, str]]` means: “you’ll receive one dictionary at a time.” Each dict maps CSV headers to cell values, which is perfect for cleaning and feature extraction steps that want named columns instead of numeric indices.",
    "",
    "---",
    "### Why `csv.DictReader` fits this pattern",
    "`csv.DictReader` already reads the file incrementally. It takes the header row as keys and turns each subsequent line into a `dict`. Combined with `yield`, this becomes a clean streaming interface: one row in, one row out.",
    "",
    "---",
    "### Why this matters for Machine Learning",
    "In ML, data is often the biggest thing in your project. If your dataset is large (logs, click data, sensor streams, scraped text, image metadata), loading everything into RAM is either slow or impossible.",
    "",
    "**This generator pattern helps because:**",
    "- **Low memory usage:** you only hold one row (or one small batch) at a time.",
    "- **Pipeline-friendly:** you can chain steps like read → clean → filter → batch without intermediate giant lists.",
    "- **Faster feedback loop:** you can start processing immediately instead of waiting for the full dataset to load.",
    "- **Matches real training workflows:** frameworks and dataloaders are built around iterating over samples/batches (one-pass streams are normal).",
    "",
    "---",
    "### Key takeaway",
    "`read_rows()` is my first reusable building block for streaming data. It’s small, but it teaches the core generator mindset: **pull data only when you need it**. That mindset scales directly into ML preprocessing and training pipelines."
  ],
  "date": "2026-02-16"
}

]
