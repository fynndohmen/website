[
{
  "title": "Timing & Logging with Python Decorators: A Tiny Tool That Scales to Machine Learning",
  "content": [
    "This small script is a practical example of why Python decorators are so powerful: you can wrap existing functions without changing their internal code, and automatically add timing and logging behavior to every call.",
    "I built two reusable decorators (timeit and log_calls). They are simple, but they mirror exactly the kind of instrumentation you want in real ML workflows: measuring slow preprocessing steps, tracking training iterations, and keeping debugging output consistent across a codebase."
  ],
  "images": [],
  "description": [
    "### What this script is",
    "The code defines two decorators and uses them to wrap arbitrary functions:",
    "- `timeit`: measures and prints execution time using `time.perf_counter()`",
    "- `log_calls`: prints a small call trace like `[CALL] train_model()`",
    "",
    "Both decorators return a new function called `wrapper` that calls the original function (`func`) and adds extra behavior around it.",
    "",
    {
      "type": "image",
      "src": "machinelearning_pics/decorators_timeit_logcalls.png",
      "alt": "Python code showing two decorators: timeit and log_calls, using TypeVar T, Callable[..., T], and functools.wraps"
    },
    "",
    "---",
    "### How the decorators work (step by step)",
    "A decorator is just a function that takes another function and returns a new function.",
    "In this script, `timeit` works like this:",
    "- It receives `func` (the original function).",
    "- It defines `wrapper(*args, **kwargs)` so it can accept any arguments.",
    "- It records timestamps around the call.",
    "- It prints the elapsed time.",
    "- It returns the original result so the decorated function behaves the same as before.",
    "",
    "That last point matters: `result = func(*args, **kwargs)` and `return result` ensure that decorating does not break the function’s API. If the original function returns something useful (a model, a loss value, a DataFrame, etc.), the wrapper passes it through unchanged.",
    "",
    "`log_calls` is similar, but instead of timing it prints a message right before the function runs.",
    "",
    "---",
    "### Why `@wraps(func)` matters",
    "Both decorators use `@wraps(func)` from `functools`. Without it, the decorated function would lose helpful metadata and appear as `wrapper` everywhere.",
    "`@wraps` preserves important attributes such as:",
    "- function name (`__name__`)",
    "- docstring (`__doc__`)",
    "- annotations (`__annotations__`)",
    "- and a `__wrapped__` link used by inspection tools",
    "",
    "This is useful because debuggers, documentation generators, and tooling that inspects functions behave much more predictably when metadata is preserved.",
    "",
    "---",
    "### Why decorators are useful in Machine Learning",
    "Machine learning code quickly turns into a pipeline: data loading, cleaning, feature engineering, batching, training, evaluation, saving, and serving. Decorators are a clean way to add cross-cutting functionality to these steps without cluttering each function.",
    "",
    "**Common ML uses where decorators shine:**",
    "- **Profiling preprocessing**: identify slow steps in data cleaning or feature computation.",
    "- **Training instrumentation**: time epochs/steps and print structured logs.",
    "- **Caching expensive work**: reuse results for feature extraction or embedding generation.",
    "- **Retry and robustness**: wrap flaky IO (downloads, remote storage reads) with retry logic.",
    "- **Experiment tracking hooks**: automatically log parameters and metrics (MLflow/W&B style).",
    "- **Guardrails**: validate shapes and dtypes before a model forward pass to fail early.",
    "",
    "The big advantage is consistency: once you have a good decorator, you can apply it everywhere and keep your pipeline readable and maintainable.",
    "",
    "---",
    "### Key takeaway",
    "This script is small, but it demonstrates a pattern that scales: decorators let you add reusable infrastructure around ML code (timing, logging, caching, validation) while keeping the model and data logic focused and uncluttered."
  ],
  "date": "2026-02-15"
},
  {
  "title": "Streaming CSV Rows with a Generator: My First `read_rows()` Data Pipeline Building Block",
  "content": [
    "This tiny function looks almost too simple, but it taught me one of the most practical lessons about generators: you can process huge datasets row-by-row without loading everything into memory.",
    "`read_rows()` is a generator that reads a CSV file lazily and yields one dictionary per row. That single idea (streaming instead of materializing) shows up everywhere in machine learning—data preprocessing, batching, training loops, and model serving."
  ],
  "images": [],
  "description": [
    "### The function",
    {
      "type": "image",
      "src": "machinelearning_pics/read_rows_generator.png",
      "alt": "Python generator function read_rows() using csv.DictReader and yield to stream CSV rows"
    },
    "",
    "---",
    "### What I learned about generators from this",
    "**1) A generator does not compute all results upfront**",
    "Because the function contains `yield`, calling `read_rows(path)` does *not* read the whole CSV. It returns a generator object, and the file is only read when someone starts iterating over it.",
    "",
    "**2) `yield` pauses execution and resumes later**",
    "Each time the loop hits `yield row`, the function pauses and hands the current row to the caller. On the next iteration, execution continues exactly after that `yield` line, reading the next CSV row.",
    "",
    "**3) It is naturally one-pass (streaming)**",
    "The generator reads the file forward, row by row. Once the iterator reaches the end, it’s done. No list of all rows is created unless I explicitly build one (like `list(read_rows(...))`).",
    "",
    "**4) The return type tells the story**",
    "`Iterator[Dict[str, str]]` means: “you’ll receive one dictionary at a time.” Each dict maps CSV headers to cell values, which is perfect for cleaning and feature extraction steps that want named columns instead of numeric indices.",
    "",
    "---",
    "### Why `csv.DictReader` fits this pattern",
    "`csv.DictReader` already reads the file incrementally. It takes the header row as keys and turns each subsequent line into a `dict`. Combined with `yield`, this becomes a clean streaming interface: one row in, one row out.",
    "",
    "---",
    "### Why this matters for Machine Learning",
    "In ML, data is often the biggest thing in your project. If your dataset is large (logs, click data, sensor streams, scraped text, image metadata), loading everything into RAM is either slow or impossible.",
    "",
    "**This generator pattern helps because:**",
    "- **Low memory usage:** you only hold one row (or one small batch) at a time.",
    "- **Pipeline-friendly:** you can chain steps like read → clean → filter → batch without intermediate giant lists.",
    "- **Faster feedback loop:** you can start processing immediately instead of waiting for the full dataset to load.",
    "- **Matches real training workflows:** frameworks and dataloaders are built around iterating over samples/batches (one-pass streams are normal).",
    "",
    "---",
    "### Key takeaway",
    "`read_rows()` is my first reusable building block for streaming data. It’s small, but it teaches the core generator mindset: **pull data only when you need it**. That mindset scales directly into ML preprocessing and training pipelines."
  ],
  "date": "2026-02-16"
},
  {
  "title": "Cleaning CSV Rows with a Dict Comprehension: Why List Comprehensions Matter in ML",
  "content": [
    "While building my CSV cleaning script, I used a compact Python feature that shows up constantly in real data workflows: comprehensions.",
    "In this part of the code I normalize whitespace across every column using a dictionary comprehension (same family as list comprehensions). It’s a small line of code, but it captures a powerful idea: transform data in a clear, readable, and efficient way."
  ],
  "images": [],
  "description": [
    "### The snippet",
    {
      "type": "image",
      "src": "machinelearning_pics/clean_row_dict_comprehension.png",
      "alt": "Python clean_row() function showing a dict comprehension that strips whitespace from all string values in a CSV row"
    },
    "",
    "---",
    "### Is this a list comprehension?",
    "Strictly speaking, the highlighted block is a **dictionary comprehension**, not a list comprehension:",
    "",
    "- **List comprehension:** `[expr for x in items if cond]` → builds a `list`",
    "- **Dict comprehension:** `{k: v for k, v in items}` → builds a `dict`",
    "",
    "They follow the same idea and syntax style, but the output container is different.",
    "",
    "---",
    "### What the dict comprehension does",
    "This block creates a new dictionary called `cleaned` from the original CSV row:",
    "",
    "- It loops through every `(k, v)` pair in `row.items()`.",
    "- If the value `v` is a string, it applies `v.strip()` to remove leading/trailing whitespace.",
    "- Otherwise, it keeps `v` unchanged.",
    "",
    "In other words, it performs a consistent normalization step across all columns, even those I didn’t explicitly name (like `email`, `city`, `department`, etc.).",
    "",
    "---",
    "### Why do this if I already stripped some fields?",
    "Earlier in the function I strip `name` and `country` (and I parse `age` and `salary`). That part is about **validation and correct parsing**.",
    "The comprehension is about **global normalization**: making sure every text column in the row is clean and predictable.",
    "",
    "Afterwards I overwrite the important fields (`name`, `country`, `age`, `salary`) with the validated/parsed values so the final row is both clean and consistent.",
    "",
    "---",
    "### Why comprehensions are useful in Machine Learning",
    "Machine learning is data-heavy, and most ML bugs are data bugs: inconsistent strings, weird whitespace, missing values, unexpected formats.",
    "Comprehensions are a practical tool for writing small, readable transformations that you apply everywhere in a pipeline.",
    "",
    "**Where they help in ML workflows:**",
    "- **Data cleaning:** trim whitespace, normalize casing, remove bad rows, map categories.",
    "- **Feature engineering:** build feature vectors, apply transformations, filter features.",
    "- **Fast prototyping:** express a transformation in one clear line instead of a verbose loop.",
    "- **Readable pipelines:** it’s easier to reason about `new = [f(x) for x in data if ok(x)]` than many lines of mutation-heavy code.",
    "",
    "One important caveat: for very large datasets, you often prefer **generator expressions** (streaming) over list comprehensions (materializing), because list comprehensions build the full list in memory. But for small-to-medium transformations, comprehensions are a great balance of clarity and speed.",
    "",
    "---",
    "### Key takeaway",
    "This single dict comprehension taught me a pattern I’ll reuse constantly: apply one consistent transformation across a whole row of data in a clean, Pythonic way. In ML, where data preprocessing is everything, being fluent with comprehensions is a real productivity boost."
  ],
  "date": "2026-02-17"
},
  {
  "title": "My Second Generator: Building a `batcher()` That Turns Streams into Mini-Batches",
  "content": [
    "After writing my first generator (`read_rows()`), I realized that streaming a dataset is only half the story. In machine learning you rarely process samples one-by-one all the way through — you usually group them into batches.",
    "`batcher()` became my second generator in this project. It takes any iterable input, collects elements into fixed-size lists, and yields those lists one at a time. Even though the code is short, it taught me a lot about generators, `TypeVar`, and why `Iterable` is often a better function input type than `Iterator`."
  ],
  "images": [],
  "description": [
    "### The generator",
    {
      "type": "image",
      "src": "machinelearning_pics/batcher_generator.png",
      "alt": "Python batcher() generator that yields list batches from an iterable input"
    },
    "",
    "---",
    "### What `batcher()` does",
    "`batcher(items, batch_size)` groups a stream of items into lists of length `batch_size`:",
    "- It appends incoming items to a list called `batch`.",
    "- Once the list reaches `batch_size`, it `yield`s that list.",
    "- Then it starts a fresh list and continues.",
    "- At the end, it yields the remaining items if the final batch isn’t full.",
    "",
    "Example: input `[1, 2, 3, 4, 5]` with `batch_size = 2` yields:",
    "- `[1, 2]`",
    "- `[3, 4]`",
    "- `[5]`",
    "",
    "---",
    "### What I learned from this generator",
    "**1) Generators aren’t only for reading files — they’re great for transforming streams**",
    "Just like `read_rows()` streams rows from disk, `batcher()` streams *batches* downstream. This keeps memory usage predictable because the program only stores the current batch list.",
    "",
    "**2) Why the parameter is `Iterable[T]` (not `Iterator[T]`)**",
    "This was an important distinction:",
    "- An **Iterable** is anything you can loop over (lists, sets, file objects, generators…).",
    "- An **Iterator** is what you get after calling `iter(...)`, and it supports `next(...)` and gets consumed.",
    "",
    "A `for` loop works with an iterable because Python automatically does:",
    "- `it = iter(items)`",
    "- then repeatedly `next(it)`",
    "",
    "So typing the input as `Iterable[T]` makes `batcher()` more flexible: it can accept both lists and generators without extra wrapping.",
    "",
    "**3) What `T` means here (and why it appears again)**",
    "`T = TypeVar(\"T\")` is a generic placeholder. In `batcher()`, it means: “whatever element type flows through this function.”",
    "",
    "- If `items` yields `int`, then `T = int` and the output is `Iterator[List[int]]`.",
    "- If `items` yields `Dict[str, str]` (like cleaned CSV rows), then `T = Dict[str, str]` and the output is `Iterator[List[Dict[str, str]]]`.",
    "",
    "This is separate from how `T` is used in my decorators — it’s the same placeholder name, but each function call gets its own inferred `T` type.",
    "",
    "**4) Why `batch: List[T]` is typed with `T`**",
    "Because the batch list should hold exactly the same thing that comes from `items`. Using `List[T]` preserves that relationship and helps the IDE/type checker understand what’s inside each batch.",
    "",
    "**5) The leftover batch matters**",
    "The final part:",
    "`if batch: yield batch`",
    "prevents silently dropping the last few elements if the dataset size isn’t divisible by the batch size. That detail is critical in real data pipelines.",
    "",
    "---",
    "### Why batching is important in Machine Learning",
    "Batching is basically the default operating mode in ML:",
    "- Neural networks train on **mini-batches** for efficiency and stable gradient updates.",
    "- Preprocessing steps often work batch-by-batch to avoid huge memory spikes.",
    "- Even outside deep learning, batching makes large-scale transformations faster and easier to manage.",
    "",
    "This generator helped me internalize the ML pipeline shape: **stream samples → group into batches → process batch-by-batch**.",
    "",
    "---",
    "### Key takeaway",
    "`batcher()` taught me how generators can be used not just to *read* data, but to *structure* a streaming pipeline. The combination of `Iterable[T]` input and `Iterator[List[T]]` output keeps the function flexible while preserving type information — exactly what you want when building ML-style data processing code."
  ],
  "date": "2026-02-18"
},
  {
  "title": "Putting It All Together: A Streaming CSV Pipeline with Decorators, Generators, and Batches",
  "content": [
    "After experimenting with decorators, generator expressions, and batching, I combined everything into one small but realistic data pipeline: read a CSV as a stream, clean each row, filter invalid data, write the output in batches, and measure what happens.",
    "This code is basically a miniature version of what many ML preprocessing scripts do: turn messy raw data into clean, model-ready data without loading the whole dataset into RAM."
  ],
  "images": [],
  "description": [
    "### The full pipeline",
    {
      "type": "image",
      "src": "machinelearning_pics/process_csv_pipeline.png",
      "alt": "Python process_csv function showing decorators, generator expressions, batching, csv.DictWriter header handling, and an assert for type safety"
    },
    "",
    "---",
    "### What this function does (high level)",
    "`process_csv(...)` is a streaming cleaner:",
    "- **Read** the input file row-by-row with `read_rows()` (generator).",
    "- **Clean** rows using `clean_row(...)` (trim whitespace, parse values, apply filters).",
    "- **Filter out** invalid rows (`None`) using a generator expression.",
    "- **Batch** the cleaned rows using `batcher(...)` so writing is done in chunks.",
    "- **Write** the cleaned rows to a new CSV with `csv.DictWriter`.",
    "- **Log + time** the whole run using decorators.",
    "",
    "The important part: it never builds a giant list of all rows — everything flows through the pipeline lazily.",
    "",
    "---",
    "### What I learned (based on the questions I asked myself)",
    "### 1) Why `-> None` is the return type",
    "At first I wondered if `-> None` means “maybe the file is empty.” But `-> None` simply means:",
    "- This function is designed to do work (write a file + print stats), not return a value.",
    "- The output is a **side effect** (the created CSV file).",
    "",
    "Even if the file is empty or all rows are invalid, it still returns `None` because returning a value is not part of the API.",
    "",
    "---",
    "### 2) Why the generator expressions are wrapped in parentheses",
    "I used:",
    "`cleaned_rows = ( clean_row(...) for r in rows )`",
    "",
    "Those parentheses don’t create a tuple — they define a **generator expression**. I mainly used them because:",
    "- the expression is written across multiple lines,",
    "- and parentheses are the clean way to format it without backslashes.",
    "",
    "---",
    "### 3) Iterable vs Iterator: why `batcher` accepts `Iterable`",
    "This was one of the most important conceptual lessons.",
    "- An **Iterable** is something you can loop over.",
    "- An **Iterator** is what actually produces elements via `next()` and gets consumed.",
    "",
    "A `for` loop works even if the object is only an iterable because Python does this under the hood:",
    "- `it = iter(items)`",
    "- then repeatedly `next(it)`",
    "",
    "So `batcher(items: Iterable[T], ...)` is more flexible: it can accept lists, sets, files, or generators. A generator is an iterator *and* an iterable, so it fits perfectly.",
    "",
    "---",
    "### 4) Why `newline=\"\"` shows up in `open(...)`",
    "I also learned that `newline=\"\"` doesn’t mean “ignore line breaks.”",
    "It means: “don’t let Python’s `open()` mess with newlines — let the `csv` module handle them.”",
    "",
    "This avoids weird formatting issues (especially on Windows), like accidentally getting blank lines between CSV rows.",
    "",
    "---",
    "### 5) Why `writer` is `Optional[...] = None` at first",
    "I don’t know the CSV header (`fieldnames`) until I see the first valid row:",
    "- The script waits for the first batch, then grabs `fieldnames = list(batch[0].keys())`.",
    "- Only then can it build `csv.DictWriter(...)` and write the header.",
    "",
    "So `writer` starts as `None` and becomes a real writer later.",
    "",
    "---",
    "### 6) Why `assert writer is not None` exists (even if it feels obvious)",
    "Logically, after writing the header, `writer` should be set.",
    "But a type checker (and even future-me refactoring the code) benefits from an explicit guarantee:",
    "- Before the `assert`, `writer` has type `Optional[DictWriter]`.",
    "- After the `assert`, the type is narrowed to `DictWriter` (not None).",
    "",
    "So the `assert` is both:",
    "- a safety net at runtime (fail fast if something is wrong),",
    "- and a signal to VS Code/type checkers that calling `writer.writerows(...)` is safe.",
    "",
    "---",
    "### 7) Why the pipeline is ML-relevant",
    "This small function mirrors real ML preprocessing patterns:",
    "- **Streaming** avoids loading huge datasets into memory.",
    "- **Cleaning + filtering** ensures models don’t train on garbage values.",
    "- **Batching** matches how ML training works (mini-batches) and keeps IO efficient.",
    "- **Instrumentation** (timing/logging) is the foundation of profiling and debugging slow pipelines.",
    "",
    "It’s not a neural network — but it’s exactly the kind of data engineering you need before you ever reach model training.",
    "",
    "---",
    "### Key takeaway",
    "By combining decorators, generator expressions, and batching, I built a pipeline that is memory-friendly, easy to reason about, and structured like real ML preprocessing code. The biggest conceptual win for me was understanding the flow: **iterable → iterator → streaming transformation → batching → writing**."
  ],
  "date": "2026-02-19"
},
{
  "title": "Adding Concurrency to My CSV Toolkit with ThreadPoolExecutor",
  "content": [
    "After building a streaming CSV cleaner with generators, parsing helpers, and batching, the next milestone on my machine learning roadmap was concurrency. Instead of processing files one-by-one, I extended my CLI so it can clean many CSV files in parallel using a controlled thread pool.",
    "This post explains how the directory-based CLI builds independent “jobs”, how ThreadPoolExecutor schedules them, and why these patterns show up everywhere in real ML pipelines (data ingestion, preprocessing, feature generation, and evaluation)."
  ],
  "images": [],
  "description": [
    "### What this script does",
    "The goal of my toolkit is simple:",
    "- Search an input directory **recursively** for `*.csv` files",
    "- For each file, create a matching output path (preserving subfolders)",
    "- Write cleaned files with a **`_cleaned` suffix**",
    "- Run multiple files **concurrently** (controlled parallelism), not just sequentially",
    "",
    "---",
    "### 1) From “one file” to “many files”: directory mode",
    "Before concurrency, my CLI processed a single CSV at a time. The first change was switching to directory mode:",
    "- `--input-dir` points to a folder that can contain nested subfolders",
    "- `rglob('*.csv')` builds the full file list recursively",
    "- For each input file, I compute an output file path that mirrors the folder structure",
    "- The output filename is derived from the input name: `file.csv` → `file_cleaned.csv`",
    "",
    "The important takeaway: **concurrency works best when each unit of work is independent**. In my case, one file = one independent job.",
    "",
    {
      "type": "image",
      "src": "machinelearning_pics/threadpool_cli_jobs.png",
      "alt": "CLI code building a recursive CSV file list and mapping each input file to an output path with a _cleaned suffix"
    },
    "",
    "---",
    "### 2) Why a “jobs list” exists at all",
    "When I started thinking about threads, my first instinct was: “Why not just loop over input files and process them?”",
    "",
    "The reason a job list is useful is that it makes the work **explicit and schedulable**:",
    "- A job contains *everything the worker needs* (input path + output path)",
    "- The thread pool can distribute jobs to workers cleanly",
    "- You can track progress and failures per job",
    "",
    "It’s basically the same idea as preparing a batch of tasks before you hand them to an execution engine.",
    "",
    "---",
    "### 3) ThreadPoolExecutor: controlled parallelism",
    "The heart of the concurrency change is `ThreadPoolExecutor`.",
    "",
    "Conceptually:",
    "- You define `max_workers` (for example `4`) → that’s the max number of jobs running at the same time",
    "- For every job, you call `executor.submit(...)`",
    "- `submit` returns a **Future**: a handle that represents “work that will finish later”",
    "",
    "I like thinking of it as a queue of tasks where only a fixed number of workers are allowed to run simultaneously.",
    "",
    {
      "type": "image",
      "src": "machinelearning_pics/threadpool_cli_executor.png",
      "alt": "ThreadPoolExecutor usage: submit jobs, map futures to jobs, iterate with as_completed, and handle errors via future.result()"
    },
    "",
    "---",
    "### 4) Futures, `future_to_job`, and why that mapping matters",
    "One confusing part at first was this idea of mapping Futures back to jobs.",
    "",
    "Key insight:",
    "- The **Future is not the result** — it’s a *container* that will eventually hold a result (or an exception).",
    "- The mapping `future_to_job` exists so I can say: “This Future belongs to *that* input/output pair.”",
    "",
    "That becomes crucial for error handling:",
    "- `future.result()` either returns normally (job succeeded)",
    "- or it raises an exception (job failed)",
    "- and because I can look up the job from the Future, I can print *which file* failed",
    "",
    "---",
    "### 5) Why `as_completed(...)` instead of a normal loop",
    "`as_completed(futures)` yields Futures **in the order they finish**, not the order they were submitted.",
    "",
    "That matters because in real workloads:",
    "- Some files are small and finish fast",
    "- Some files are huge and finish later",
    "",
    "With `as_completed`, I get progress feedback as soon as jobs complete, and failures surface earlier instead of being hidden behind one slow file.",
    "",
    "---",
    "### 6) What I learned about concurrency (the practical version)",
    "- **Concurrency** means *overlapping work*: while one job is waiting (e.g., on disk I/O), another job can run.",
    "- A thread pool gives **controlled parallelism**: not 200 threads, but a stable number like 4 or 8.",
    "- You want to avoid shared state: each job writes to its own output file → fewer locks, fewer bugs.",
    "- Futures are the clean way to handle results and exceptions in parallel code.",
    "",
    "---",
    "### 7) Why this matters for machine learning",
    "Even before training models, ML workflows are full of “many independent tasks”:",
    "- preprocessing many files (logs, images, shards)",
    "- feature extraction per file / per chunk",
    "- dataset validation / cleaning",
    "- running evaluation across many checkpoints or parameter configs",
    "",
    "If you can express your work as **independent jobs** and process them with a controlled executor, you get:",
    "- faster iteration loops",
    "- more scalable data pipelines",
    "- and a strong foundation for later (multiprocessing, distributed systems, and MLOps tooling).",
    "",
    "---",
    "### Next steps",
    "Next I want to benchmark different thread counts (1 vs 4 vs 8) and learn how to recognize when I’m I/O-limited vs CPU-limited — because that’s the difference between “threads help a lot” and “threads don’t help at all”."
  ],
  "date": "2026-02-20"
}






]
