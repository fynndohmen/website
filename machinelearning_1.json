[
{
  "title": "Timing & Logging with Python Decorators: A Tiny Tool That Scales to Machine Learning",
  "content": [
    "This small script is a practical example of why Python decorators are so powerful: you can wrap existing functions without changing their internal code, and automatically add timing and logging behavior to every call.",
    "I built two reusable decorators (timeit and log_calls). They are simple, but they mirror exactly the kind of instrumentation you want in real ML workflows: measuring slow preprocessing steps, tracking training iterations, and keeping debugging output consistent across a codebase."
  ],
  "images": [],
  "description": [
    "### What this script is",
    "The code defines two decorators and uses them to wrap arbitrary functions:",
    "- `timeit`: measures and prints execution time using `time.perf_counter()`",
    "- `log_calls`: prints a small call trace like `[CALL] train_model()`",
    "",
    "Both decorators return a new function called `wrapper` that calls the original function (`func`) and adds extra behavior around it.",
    "",
    {
      "type": "image",
      "src": "machinelearning_pics/decorators_timeit_logcalls.png",
      "alt": "Python code showing two decorators: timeit and log_calls, using TypeVar T, Callable[..., T], and functools.wraps"
    },
    "",
    "---",
    "### How the decorators work (step by step)",
    "A decorator is just a function that takes another function and returns a new function.",
    "In this script, `timeit` works like this:",
    "- It receives `func` (the original function).",
    "- It defines `wrapper(*args, **kwargs)` so it can accept any arguments.",
    "- It records timestamps around the call.",
    "- It prints the elapsed time.",
    "- It returns the original result so the decorated function behaves the same as before.",
    "",
    "That last point matters: `result = func(*args, **kwargs)` and `return result` ensure that decorating does not break the function’s API. If the original function returns something useful (a model, a loss value, a DataFrame, etc.), the wrapper passes it through unchanged.",
    "",
    "`log_calls` is similar, but instead of timing it prints a message right before the function runs.",
    "",
    "---",
    "### Why `@wraps(func)` matters",
    "Both decorators use `@wraps(func)` from `functools`. Without it, the decorated function would lose helpful metadata and appear as `wrapper` everywhere.",
    "`@wraps` preserves important attributes such as:",
    "- function name (`__name__`)",
    "- docstring (`__doc__`)",
    "- annotations (`__annotations__`)",
    "- and a `__wrapped__` link used by inspection tools",
    "",
    "This is useful because debuggers, documentation generators, and tooling that inspects functions behave much more predictably when metadata is preserved.",
    "",
    "---",
    "### Why decorators are useful in Machine Learning",
    "Machine learning code quickly turns into a pipeline: data loading, cleaning, feature engineering, batching, training, evaluation, saving, and serving. Decorators are a clean way to add cross-cutting functionality to these steps without cluttering each function.",
    "",
    "**Common ML uses where decorators shine:**",
    "- **Profiling preprocessing**: identify slow steps in data cleaning or feature computation.",
    "- **Training instrumentation**: time epochs/steps and print structured logs.",
    "- **Caching expensive work**: reuse results for feature extraction or embedding generation.",
    "- **Retry and robustness**: wrap flaky IO (downloads, remote storage reads) with retry logic.",
    "- **Experiment tracking hooks**: automatically log parameters and metrics (MLflow/W&B style).",
    "- **Guardrails**: validate shapes and dtypes before a model forward pass to fail early.",
    "",
    "The big advantage is consistency: once you have a good decorator, you can apply it everywhere and keep your pipeline readable and maintainable.",
    "",
    "---",
    "### Key takeaway",
    "This script is small, but it demonstrates a pattern that scales: decorators let you add reusable infrastructure around ML code (timing, logging, caching, validation) while keeping the model and data logic focused and uncluttered."
  ],
  "date": "2026-02-15"
},
  {
  "title": "Streaming CSV Rows with a Generator: My First `read_rows()` Data Pipeline Building Block",
  "content": [
    "This tiny function looks almost too simple, but it taught me one of the most practical lessons about generators: you can process huge datasets row-by-row without loading everything into memory.",
    "`read_rows()` is a generator that reads a CSV file lazily and yields one dictionary per row. That single idea (streaming instead of materializing) shows up everywhere in machine learning—data preprocessing, batching, training loops, and model serving."
  ],
  "images": [],
  "description": [
    "### The function",
    {
      "type": "image",
      "src": "machinelearning_pics/read_rows_generator.png",
      "alt": "Python generator function read_rows() using csv.DictReader and yield to stream CSV rows"
    },
    "",
    "---",
    "### What I learned about generators from this",
    "**1) A generator does not compute all results upfront**",
    "Because the function contains `yield`, calling `read_rows(path)` does *not* read the whole CSV. It returns a generator object, and the file is only read when someone starts iterating over it.",
    "",
    "**2) `yield` pauses execution and resumes later**",
    "Each time the loop hits `yield row`, the function pauses and hands the current row to the caller. On the next iteration, execution continues exactly after that `yield` line, reading the next CSV row.",
    "",
    "**3) It is naturally one-pass (streaming)**",
    "The generator reads the file forward, row by row. Once the iterator reaches the end, it’s done. No list of all rows is created unless I explicitly build one (like `list(read_rows(...))`).",
    "",
    "**4) The return type tells the story**",
    "`Iterator[Dict[str, str]]` means: “you’ll receive one dictionary at a time.” Each dict maps CSV headers to cell values, which is perfect for cleaning and feature extraction steps that want named columns instead of numeric indices.",
    "",
    "---",
    "### Why `csv.DictReader` fits this pattern",
    "`csv.DictReader` already reads the file incrementally. It takes the header row as keys and turns each subsequent line into a `dict`. Combined with `yield`, this becomes a clean streaming interface: one row in, one row out.",
    "",
    "---",
    "### Why this matters for Machine Learning",
    "In ML, data is often the biggest thing in your project. If your dataset is large (logs, click data, sensor streams, scraped text, image metadata), loading everything into RAM is either slow or impossible.",
    "",
    "**This generator pattern helps because:**",
    "- **Low memory usage:** you only hold one row (or one small batch) at a time.",
    "- **Pipeline-friendly:** you can chain steps like read → clean → filter → batch without intermediate giant lists.",
    "- **Faster feedback loop:** you can start processing immediately instead of waiting for the full dataset to load.",
    "- **Matches real training workflows:** frameworks and dataloaders are built around iterating over samples/batches (one-pass streams are normal).",
    "",
    "---",
    "### Key takeaway",
    "`read_rows()` is my first reusable building block for streaming data. It’s small, but it teaches the core generator mindset: **pull data only when you need it**. That mindset scales directly into ML preprocessing and training pipelines."
  ],
  "date": "2026-02-16"
},
  {
  "title": "Cleaning CSV Rows with a Dict Comprehension: Why List Comprehensions Matter in ML",
  "content": [
    "While building my CSV cleaning script, I used a compact Python feature that shows up constantly in real data workflows: comprehensions.",
    "In this part of the code I normalize whitespace across every column using a dictionary comprehension (same family as list comprehensions). It’s a small line of code, but it captures a powerful idea: transform data in a clear, readable, and efficient way."
  ],
  "images": [],
  "description": [
    "### The snippet",
    {
      "type": "image",
      "src": "machinelearning_pics/clean_row_dict_comprehension.png",
      "alt": "Python clean_row() function showing a dict comprehension that strips whitespace from all string values in a CSV row"
    },
    "",
    "---",
    "### Is this a list comprehension?",
    "Strictly speaking, the highlighted block is a **dictionary comprehension**, not a list comprehension:",
    "",
    "- **List comprehension:** `[expr for x in items if cond]` → builds a `list`",
    "- **Dict comprehension:** `{k: v for k, v in items}` → builds a `dict`",
    "",
    "They follow the same idea and syntax style, but the output container is different.",
    "",
    "---",
    "### What the dict comprehension does",
    "This block creates a new dictionary called `cleaned` from the original CSV row:",
    "",
    "- It loops through every `(k, v)` pair in `row.items()`.",
    "- If the value `v` is a string, it applies `v.strip()` to remove leading/trailing whitespace.",
    "- Otherwise, it keeps `v` unchanged.",
    "",
    "In other words, it performs a consistent normalization step across all columns, even those I didn’t explicitly name (like `email`, `city`, `department`, etc.).",
    "",
    "---",
    "### Why do this if I already stripped some fields?",
    "Earlier in the function I strip `name` and `country` (and I parse `age` and `salary`). That part is about **validation and correct parsing**.",
    "The comprehension is about **global normalization**: making sure every text column in the row is clean and predictable.",
    "",
    "Afterwards I overwrite the important fields (`name`, `country`, `age`, `salary`) with the validated/parsed values so the final row is both clean and consistent.",
    "",
    "---",
    "### Why comprehensions are useful in Machine Learning",
    "Machine learning is data-heavy, and most ML bugs are data bugs: inconsistent strings, weird whitespace, missing values, unexpected formats.",
    "Comprehensions are a practical tool for writing small, readable transformations that you apply everywhere in a pipeline.",
    "",
    "**Where they help in ML workflows:**",
    "- **Data cleaning:** trim whitespace, normalize casing, remove bad rows, map categories.",
    "- **Feature engineering:** build feature vectors, apply transformations, filter features.",
    "- **Fast prototyping:** express a transformation in one clear line instead of a verbose loop.",
    "- **Readable pipelines:** it’s easier to reason about `new = [f(x) for x in data if ok(x)]` than many lines of mutation-heavy code.",
    "",
    "One important caveat: for very large datasets, you often prefer **generator expressions** (streaming) over list comprehensions (materializing), because list comprehensions build the full list in memory. But for small-to-medium transformations, comprehensions are a great balance of clarity and speed.",
    "",
    "---",
    "### Key takeaway",
    "This single dict comprehension taught me a pattern I’ll reuse constantly: apply one consistent transformation across a whole row of data in a clean, Pythonic way. In ML, where data preprocessing is everything, being fluent with comprehensions is a real productivity boost."
  ],
  "date": "2026-02-17"
},
  {
  "title": "My Second Generator: Building a `batcher()` That Turns Streams into Mini-Batches",
  "content": [
    "After writing my first generator (`read_rows()`), I realized that streaming a dataset is only half the story. In machine learning you rarely process samples one-by-one all the way through — you usually group them into batches.",
    "`batcher()` became my second generator in this project. It takes any iterable input, collects elements into fixed-size lists, and yields those lists one at a time. Even though the code is short, it taught me a lot about generators, `TypeVar`, and why `Iterable` is often a better function input type than `Iterator`."
  ],
  "images": [],
  "description": [
    "### The generator",
    {
      "type": "image",
      "src": "machinelearning_pics/batcher_generator.png",
      "alt": "Python batcher() generator that yields list batches from an iterable input"
    },
    "",
    "---",
    "### What `batcher()` does",
    "`batcher(items, batch_size)` groups a stream of items into lists of length `batch_size`:",
    "- It appends incoming items to a list called `batch`.",
    "- Once the list reaches `batch_size`, it `yield`s that list.",
    "- Then it starts a fresh list and continues.",
    "- At the end, it yields the remaining items if the final batch isn’t full.",
    "",
    "Example: input `[1, 2, 3, 4, 5]` with `batch_size = 2` yields:",
    "- `[1, 2]`",
    "- `[3, 4]`",
    "- `[5]`",
    "",
    "---",
    "### What I learned from this generator",
    "**1) Generators aren’t only for reading files — they’re great for transforming streams**",
    "Just like `read_rows()` streams rows from disk, `batcher()` streams *batches* downstream. This keeps memory usage predictable because the program only stores the current batch list.",
    "",
    "**2) Why the parameter is `Iterable[T]` (not `Iterator[T]`)**",
    "This was an important distinction:",
    "- An **Iterable** is anything you can loop over (lists, sets, file objects, generators…).",
    "- An **Iterator** is what you get after calling `iter(...)`, and it supports `next(...)` and gets consumed.",
    "",
    "A `for` loop works with an iterable because Python automatically does:",
    "- `it = iter(items)`",
    "- then repeatedly `next(it)`",
    "",
    "So typing the input as `Iterable[T]` makes `batcher()` more flexible: it can accept both lists and generators without extra wrapping.",
    "",
    "**3) What `T` means here (and why it appears again)**",
    "`T = TypeVar(\"T\")` is a generic placeholder. In `batcher()`, it means: “whatever element type flows through this function.”",
    "",
    "- If `items` yields `int`, then `T = int` and the output is `Iterator[List[int]]`.",
    "- If `items` yields `Dict[str, str]` (like cleaned CSV rows), then `T = Dict[str, str]` and the output is `Iterator[List[Dict[str, str]]]`.",
    "",
    "This is separate from how `T` is used in my decorators — it’s the same placeholder name, but each function call gets its own inferred `T` type.",
    "",
    "**4) Why `batch: List[T]` is typed with `T`**",
    "Because the batch list should hold exactly the same thing that comes from `items`. Using `List[T]` preserves that relationship and helps the IDE/type checker understand what’s inside each batch.",
    "",
    "**5) The leftover batch matters**",
    "The final part:",
    "`if batch: yield batch`",
    "prevents silently dropping the last few elements if the dataset size isn’t divisible by the batch size. That detail is critical in real data pipelines.",
    "",
    "---",
    "### Why batching is important in Machine Learning",
    "Batching is basically the default operating mode in ML:",
    "- Neural networks train on **mini-batches** for efficiency and stable gradient updates.",
    "- Preprocessing steps often work batch-by-batch to avoid huge memory spikes.",
    "- Even outside deep learning, batching makes large-scale transformations faster and easier to manage.",
    "",
    "This generator helped me internalize the ML pipeline shape: **stream samples → group into batches → process batch-by-batch**.",
    "",
    "---",
    "### Key takeaway",
    "`batcher()` taught me how generators can be used not just to *read* data, but to *structure* a streaming pipeline. The combination of `Iterable[T]` input and `Iterator[List[T]]` output keeps the function flexible while preserving type information — exactly what you want when building ML-style data processing code."
  ],
  "date": "2026-02-18"
}



]
