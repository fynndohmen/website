[
{
  "title": "Timing & Logging with Python Decorators: A Tiny Tool That Scales to Machine Learning",
  "content": [
    "This small script is a practical example of why Python decorators are so powerful: you can wrap existing functions without changing their internal code, and automatically add timing and logging behavior to every call.",
    "I built two reusable decorators (timeit and log_calls). They are simple, but they mirror exactly the kind of instrumentation you want in real ML workflows: measuring slow preprocessing steps, tracking training iterations, and keeping debugging output consistent across a codebase."
  ],
  "images": [],
  "description": [
    "### What this script is",
    "The code defines two decorators and uses them to wrap arbitrary functions:",
    "- `timeit`: measures and prints execution time using `time.perf_counter()`",
    "- `log_calls`: prints a small call trace like `[CALL] train_model()`",
    "",
    "Both decorators return a new function called `wrapper` that calls the original function (`func`) and adds extra behavior around it.",
    "",
    {
      "type": "image",
      "src": "machinelearning_pics/decorators_timeit_logcalls.png",
      "alt": "Python code showing two decorators: timeit and log_calls, using TypeVar T, Callable[..., T], and functools.wraps"
    },
    "",
    "---",
    "### How the decorators work (step by step)",
    "A decorator is just a function that takes another function and returns a new function.",
    "In this script, `timeit` works like this:",
    "- It receives `func` (the original function).",
    "- It defines `wrapper(*args, **kwargs)` so it can accept any arguments.",
    "- It records timestamps around the call.",
    "- It prints the elapsed time.",
    "- It returns the original result so the decorated function behaves the same as before.",
    "",
    "That last point matters: `result = func(*args, **kwargs)` and `return result` ensure that decorating does not break the function’s API. If the original function returns something useful (a model, a loss value, a DataFrame, etc.), the wrapper passes it through unchanged.",
    "",
    "`log_calls` is similar, but instead of timing it prints a message right before the function runs.",
    "",
    "---",
    "### Why `@wraps(func)` matters",
    "Both decorators use `@wraps(func)` from `functools`. Without it, the decorated function would lose helpful metadata and appear as `wrapper` everywhere.",
    "`@wraps` preserves important attributes such as:",
    "- function name (`__name__`)",
    "- docstring (`__doc__`)",
    "- annotations (`__annotations__`)",
    "- and a `__wrapped__` link used by inspection tools",
    "",
    "This is useful because debuggers, documentation generators, and tooling that inspects functions behave much more predictably when metadata is preserved.",
    "",
    "---",
    "### Why decorators are useful in Machine Learning",
    "Machine learning code quickly turns into a pipeline: data loading, cleaning, feature engineering, batching, training, evaluation, saving, and serving. Decorators are a clean way to add cross-cutting functionality to these steps without cluttering each function.",
    "",
    "**Common ML uses where decorators shine:**",
    "- **Profiling preprocessing**: identify slow steps in data cleaning or feature computation.",
    "- **Training instrumentation**: time epochs/steps and print structured logs.",
    "- **Caching expensive work**: reuse results for feature extraction or embedding generation.",
    "- **Retry and robustness**: wrap flaky IO (downloads, remote storage reads) with retry logic.",
    "- **Experiment tracking hooks**: automatically log parameters and metrics (MLflow/W&B style).",
    "- **Guardrails**: validate shapes and dtypes before a model forward pass to fail early.",
    "",
    "The big advantage is consistency: once you have a good decorator, you can apply it everywhere and keep your pipeline readable and maintainable.",
    "",
    "---",
    "### Key takeaway",
    "This script is small, but it demonstrates a pattern that scales: decorators let you add reusable infrastructure around ML code (timing, logging, caching, validation) while keeping the model and data logic focused and uncluttered."
  ],
  "date": "2026-02-15"
},
  {
  "title": "Streaming CSV Rows with a Generator: My First `read_rows()` Data Pipeline Building Block",
  "content": [
    "This tiny function looks almost too simple, but it taught me one of the most practical lessons about generators: you can process huge datasets row-by-row without loading everything into memory.",
    "`read_rows()` is a generator that reads a CSV file lazily and yields one dictionary per row. That single idea (streaming instead of materializing) shows up everywhere in machine learning—data preprocessing, batching, training loops, and model serving."
  ],
  "images": [],
  "description": [
    "### The function",
    {
      "type": "image",
      "src": "machinelearning_pics/read_rows_generator.png",
      "alt": "Python generator function read_rows() using csv.DictReader and yield to stream CSV rows"
    },
    "",
    "---",
    "### What I learned about generators from this",
    "**1) A generator does not compute all results upfront**",
    "Because the function contains `yield`, calling `read_rows(path)` does *not* read the whole CSV. It returns a generator object, and the file is only read when someone starts iterating over it.",
    "",
    "**2) `yield` pauses execution and resumes later**",
    "Each time the loop hits `yield row`, the function pauses and hands the current row to the caller. On the next iteration, execution continues exactly after that `yield` line, reading the next CSV row.",
    "",
    "**3) It is naturally one-pass (streaming)**",
    "The generator reads the file forward, row by row. Once the iterator reaches the end, it’s done. No list of all rows is created unless I explicitly build one (like `list(read_rows(...))`).",
    "",
    "**4) The return type tells the story**",
    "`Iterator[Dict[str, str]]` means: “you’ll receive one dictionary at a time.” Each dict maps CSV headers to cell values, which is perfect for cleaning and feature extraction steps that want named columns instead of numeric indices.",
    "",
    "---",
    "### Why `csv.DictReader` fits this pattern",
    "`csv.DictReader` already reads the file incrementally. It takes the header row as keys and turns each subsequent line into a `dict`. Combined with `yield`, this becomes a clean streaming interface: one row in, one row out.",
    "",
    "---",
    "### Why this matters for Machine Learning",
    "In ML, data is often the biggest thing in your project. If your dataset is large (logs, click data, sensor streams, scraped text, image metadata), loading everything into RAM is either slow or impossible.",
    "",
    "**This generator pattern helps because:**",
    "- **Low memory usage:** you only hold one row (or one small batch) at a time.",
    "- **Pipeline-friendly:** you can chain steps like read → clean → filter → batch without intermediate giant lists.",
    "- **Faster feedback loop:** you can start processing immediately instead of waiting for the full dataset to load.",
    "- **Matches real training workflows:** frameworks and dataloaders are built around iterating over samples/batches (one-pass streams are normal).",
    "",
    "---",
    "### Key takeaway",
    "`read_rows()` is my first reusable building block for streaming data. It’s small, but it teaches the core generator mindset: **pull data only when you need it**. That mindset scales directly into ML preprocessing and training pipelines."
  ],
  "date": "2026-02-16"
},
  {
  "title": "Cleaning CSV Rows with a Dict Comprehension: Why List Comprehensions Matter in ML",
  "content": [
    "While building my CSV cleaning script, I used a compact Python feature that shows up constantly in real data workflows: comprehensions.",
    "In this part of the code I normalize whitespace across every column using a dictionary comprehension (same family as list comprehensions). It’s a small line of code, but it captures a powerful idea: transform data in a clear, readable, and efficient way."
  ],
  "images": [],
  "description": [
    "### The snippet",
    {
      "type": "image",
      "src": "machinelearning_pics/clean_row_dict_comprehension.png",
      "alt": "Python clean_row() function showing a dict comprehension that strips whitespace from all string values in a CSV row"
    },
    "",
    "---",
    "### Is this a list comprehension?",
    "Strictly speaking, the highlighted block is a **dictionary comprehension**, not a list comprehension:",
    "",
    "- **List comprehension:** `[expr for x in items if cond]` → builds a `list`",
    "- **Dict comprehension:** `{k: v for k, v in items}` → builds a `dict`",
    "",
    "They follow the same idea and syntax style, but the output container is different.",
    "",
    "---",
    "### What the dict comprehension does",
    "This block creates a new dictionary called `cleaned` from the original CSV row:",
    "",
    "- It loops through every `(k, v)` pair in `row.items()`.",
    "- If the value `v` is a string, it applies `v.strip()` to remove leading/trailing whitespace.",
    "- Otherwise, it keeps `v` unchanged.",
    "",
    "In other words, it performs a consistent normalization step across all columns, even those I didn’t explicitly name (like `email`, `city`, `department`, etc.).",
    "",
    "---",
    "### Why do this if I already stripped some fields?",
    "Earlier in the function I strip `name` and `country` (and I parse `age` and `salary`). That part is about **validation and correct parsing**.",
    "The comprehension is about **global normalization**: making sure every text column in the row is clean and predictable.",
    "",
    "Afterwards I overwrite the important fields (`name`, `country`, `age`, `salary`) with the validated/parsed values so the final row is both clean and consistent.",
    "",
    "---",
    "### Why comprehensions are useful in Machine Learning",
    "Machine learning is data-heavy, and most ML bugs are data bugs: inconsistent strings, weird whitespace, missing values, unexpected formats.",
    "Comprehensions are a practical tool for writing small, readable transformations that you apply everywhere in a pipeline.",
    "",
    "**Where they help in ML workflows:**",
    "- **Data cleaning:** trim whitespace, normalize casing, remove bad rows, map categories.",
    "- **Feature engineering:** build feature vectors, apply transformations, filter features.",
    "- **Fast prototyping:** express a transformation in one clear line instead of a verbose loop.",
    "- **Readable pipelines:** it’s easier to reason about `new = [f(x) for x in data if ok(x)]` than many lines of mutation-heavy code.",
    "",
    "One important caveat: for very large datasets, you often prefer **generator expressions** (streaming) over list comprehensions (materializing), because list comprehensions build the full list in memory. But for small-to-medium transformations, comprehensions are a great balance of clarity and speed.",
    "",
    "---",
    "### Key takeaway",
    "This single dict comprehension taught me a pattern I’ll reuse constantly: apply one consistent transformation across a whole row of data in a clean, Pythonic way. In ML, where data preprocessing is everything, being fluent with comprehensions is a real productivity boost."
  ],
  "date": "2026-02-17"
},
  {
  "title": "My Second Generator: Building a `batcher()` That Turns Streams into Mini-Batches",
  "content": [
    "After writing my first generator (`read_rows()`), I realized that streaming a dataset is only half the story. In machine learning you rarely process samples one-by-one all the way through — you usually group them into batches.",
    "`batcher()` became my second generator in this project. It takes any iterable input, collects elements into fixed-size lists, and yields those lists one at a time. Even though the code is short, it taught me a lot about generators, `TypeVar`, and why `Iterable` is often a better function input type than `Iterator`."
  ],
  "images": [],
  "description": [
    "### The generator",
    {
      "type": "image",
      "src": "machinelearning_pics/batcher_generator.png",
      "alt": "Python batcher() generator that yields list batches from an iterable input"
    },
    "",
    "---",
    "### What `batcher()` does",
    "`batcher(items, batch_size)` groups a stream of items into lists of length `batch_size`:",
    "- It appends incoming items to a list called `batch`.",
    "- Once the list reaches `batch_size`, it `yield`s that list.",
    "- Then it starts a fresh list and continues.",
    "- At the end, it yields the remaining items if the final batch isn’t full.",
    "",
    "Example: input `[1, 2, 3, 4, 5]` with `batch_size = 2` yields:",
    "- `[1, 2]`",
    "- `[3, 4]`",
    "- `[5]`",
    "",
    "---",
    "### What I learned from this generator",
    "**1) Generators aren’t only for reading files — they’re great for transforming streams**",
    "Just like `read_rows()` streams rows from disk, `batcher()` streams *batches* downstream. This keeps memory usage predictable because the program only stores the current batch list.",
    "",
    "**2) Why the parameter is `Iterable[T]` (not `Iterator[T]`)**",
    "This was an important distinction:",
    "- An **Iterable** is anything you can loop over (lists, sets, file objects, generators…).",
    "- An **Iterator** is what you get after calling `iter(...)`, and it supports `next(...)` and gets consumed.",
    "",
    "A `for` loop works with an iterable because Python automatically does:",
    "- `it = iter(items)`",
    "- then repeatedly `next(it)`",
    "",
    "So typing the input as `Iterable[T]` makes `batcher()` more flexible: it can accept both lists and generators without extra wrapping.",
    "",
    "**3) What `T` means here (and why it appears again)**",
    "`T = TypeVar(\"T\")` is a generic placeholder. In `batcher()`, it means: “whatever element type flows through this function.”",
    "",
    "- If `items` yields `int`, then `T = int` and the output is `Iterator[List[int]]`.",
    "- If `items` yields `Dict[str, str]` (like cleaned CSV rows), then `T = Dict[str, str]` and the output is `Iterator[List[Dict[str, str]]]`.",
    "",
    "This is separate from how `T` is used in my decorators — it’s the same placeholder name, but each function call gets its own inferred `T` type.",
    "",
    "**4) Why `batch: List[T]` is typed with `T`**",
    "Because the batch list should hold exactly the same thing that comes from `items`. Using `List[T]` preserves that relationship and helps the IDE/type checker understand what’s inside each batch.",
    "",
    "**5) The leftover batch matters**",
    "The final part:",
    "`if batch: yield batch`",
    "prevents silently dropping the last few elements if the dataset size isn’t divisible by the batch size. That detail is critical in real data pipelines.",
    "",
    "---",
    "### Why batching is important in Machine Learning",
    "Batching is basically the default operating mode in ML:",
    "- Neural networks train on **mini-batches** for efficiency and stable gradient updates.",
    "- Preprocessing steps often work batch-by-batch to avoid huge memory spikes.",
    "- Even outside deep learning, batching makes large-scale transformations faster and easier to manage.",
    "",
    "This generator helped me internalize the ML pipeline shape: **stream samples → group into batches → process batch-by-batch**.",
    "",
    "---",
    "### Key takeaway",
    "`batcher()` taught me how generators can be used not just to *read* data, but to *structure* a streaming pipeline. The combination of `Iterable[T]` input and `Iterator[List[T]]` output keeps the function flexible while preserving type information — exactly what you want when building ML-style data processing code."
  ],
  "date": "2026-02-18"
},
  {
  "title": "Putting It All Together: A Streaming CSV Pipeline with Decorators, Generators, and Batches",
  "content": [
    "After experimenting with decorators, generator expressions, and batching, I combined everything into one small but realistic data pipeline: read a CSV as a stream, clean each row, filter invalid data, write the output in batches, and measure what happens.",
    "This code is basically a miniature version of what many ML preprocessing scripts do: turn messy raw data into clean, model-ready data without loading the whole dataset into RAM."
  ],
  "images": [],
  "description": [
    "### The full pipeline",
    {
      "type": "image",
      "src": "machinelearning_pics/process_csv_pipeline.png",
      "alt": "Python process_csv function showing decorators, generator expressions, batching, csv.DictWriter header handling, and an assert for type safety"
    },
    "",
    "---",
    "### What this function does (high level)",
    "`process_csv(...)` is a streaming cleaner:",
    "- **Read** the input file row-by-row with `read_rows()` (generator).",
    "- **Clean** rows using `clean_row(...)` (trim whitespace, parse values, apply filters).",
    "- **Filter out** invalid rows (`None`) using a generator expression.",
    "- **Batch** the cleaned rows using `batcher(...)` so writing is done in chunks.",
    "- **Write** the cleaned rows to a new CSV with `csv.DictWriter`.",
    "- **Log + time** the whole run using decorators.",
    "",
    "The important part: it never builds a giant list of all rows — everything flows through the pipeline lazily.",
    "",
    "---",
    "### What I learned (based on the questions I asked myself)",
    "### 1) Why `-> None` is the return type",
    "At first I wondered if `-> None` means “maybe the file is empty.” But `-> None` simply means:",
    "- This function is designed to do work (write a file + print stats), not return a value.",
    "- The output is a **side effect** (the created CSV file).",
    "",
    "Even if the file is empty or all rows are invalid, it still returns `None` because returning a value is not part of the API.",
    "",
    "---",
    "### 2) Why the generator expressions are wrapped in parentheses",
    "I used:",
    "`cleaned_rows = ( clean_row(...) for r in rows )`",
    "",
    "Those parentheses don’t create a tuple — they define a **generator expression**. I mainly used them because:",
    "- the expression is written across multiple lines,",
    "- and parentheses are the clean way to format it without backslashes.",
    "",
    "---",
    "### 3) Iterable vs Iterator: why `batcher` accepts `Iterable`",
    "This was one of the most important conceptual lessons.",
    "- An **Iterable** is something you can loop over.",
    "- An **Iterator** is what actually produces elements via `next()` and gets consumed.",
    "",
    "A `for` loop works even if the object is only an iterable because Python does this under the hood:",
    "- `it = iter(items)`",
    "- then repeatedly `next(it)`",
    "",
    "So `batcher(items: Iterable[T], ...)` is more flexible: it can accept lists, sets, files, or generators. A generator is an iterator *and* an iterable, so it fits perfectly.",
    "",
    "---",
    "### 4) Why `newline=\"\"` shows up in `open(...)`",
    "I also learned that `newline=\"\"` doesn’t mean “ignore line breaks.”",
    "It means: “don’t let Python’s `open()` mess with newlines — let the `csv` module handle them.”",
    "",
    "This avoids weird formatting issues (especially on Windows), like accidentally getting blank lines between CSV rows.",
    "",
    "---",
    "### 5) Why `writer` is `Optional[...] = None` at first",
    "I don’t know the CSV header (`fieldnames`) until I see the first valid row:",
    "- The script waits for the first batch, then grabs `fieldnames = list(batch[0].keys())`.",
    "- Only then can it build `csv.DictWriter(...)` and write the header.",
    "",
    "So `writer` starts as `None` and becomes a real writer later.",
    "",
    "---",
    "### 6) Why `assert writer is not None` exists (even if it feels obvious)",
    "Logically, after writing the header, `writer` should be set.",
    "But a type checker (and even future-me refactoring the code) benefits from an explicit guarantee:",
    "- Before the `assert`, `writer` has type `Optional[DictWriter]`.",
    "- After the `assert`, the type is narrowed to `DictWriter` (not None).",
    "",
    "So the `assert` is both:",
    "- a safety net at runtime (fail fast if something is wrong),",
    "- and a signal to VS Code/type checkers that calling `writer.writerows(...)` is safe.",
    "",
    "---",
    "### 7) Why the pipeline is ML-relevant",
    "This small function mirrors real ML preprocessing patterns:",
    "- **Streaming** avoids loading huge datasets into memory.",
    "- **Cleaning + filtering** ensures models don’t train on garbage values.",
    "- **Batching** matches how ML training works (mini-batches) and keeps IO efficient.",
    "- **Instrumentation** (timing/logging) is the foundation of profiling and debugging slow pipelines.",
    "",
    "It’s not a neural network — but it’s exactly the kind of data engineering you need before you ever reach model training.",
    "",
    "---",
    "### Key takeaway",
    "By combining decorators, generator expressions, and batching, I built a pipeline that is memory-friendly, easy to reason about, and structured like real ML preprocessing code. The biggest conceptual win for me was understanding the flow: **iterable → iterator → streaming transformation → batching → writing**."
  ],
  "date": "2026-02-19"
},
{
  "title": "Adding Concurrency to My CSV Toolkit with ThreadPoolExecutor",
  "content": [
    "After building a streaming CSV cleaner with generators, parsing helpers, and batching, the next milestone on my machine learning roadmap was concurrency. Instead of processing files one-by-one, I extended my CLI so it can clean many CSV files in parallel using a controlled thread pool.",
    "This post explains how the directory-based CLI builds independent “jobs”, how ThreadPoolExecutor schedules them, and why these patterns show up everywhere in real ML pipelines (data ingestion, preprocessing, feature generation, and evaluation)."
  ],
  "images": [],
  "description": [
    "### What this script does",
    "The goal of my toolkit is simple:",
    "- Search an input directory **recursively** for `*.csv` files",
    "- For each file, create a matching output path (preserving subfolders)",
    "- Write cleaned files with a **`_cleaned` suffix**",
    "- Run multiple files **concurrently** (controlled parallelism), not just sequentially",
    "",
    "---",
    "### 1) From “one file” to “many files”: directory mode",
    "Before concurrency, my CLI processed a single CSV at a time. The first change was switching to directory mode:",
    "- `--input-dir` points to a folder that can contain nested subfolders",
    "- `rglob('*.csv')` builds the full file list recursively",
    "- For each input file, I compute an output file path that mirrors the folder structure",
    "- The output filename is derived from the input name: `file.csv` → `file_cleaned.csv`",
    "",
    "The important takeaway: **concurrency works best when each unit of work is independent**. In my case, one file = one independent job.",
    "",
    {
      "type": "image",
      "src": "machinelearning_pics/threadpool_cli_jobs.png",
      "alt": "CLI code building a recursive CSV file list and mapping each input file to an output path with a _cleaned suffix"
    },
    "",
    "---",
    "### 2) Why a “jobs list” exists at all",
    "When I started thinking about threads, my first instinct was: “Why not just loop over input files and process them?”",
    "",
    "The reason a job list is useful is that it makes the work **explicit and schedulable**:",
    "- A job contains *everything the worker needs* (input path + output path)",
    "- The thread pool can distribute jobs to workers cleanly",
    "- You can track progress and failures per job",
    "",
    "It’s basically the same idea as preparing a batch of tasks before you hand them to an execution engine.",
    "",
    "---",
    "### 3) ThreadPoolExecutor: controlled parallelism",
    "The heart of the concurrency change is `ThreadPoolExecutor`.",
    "",
    "Conceptually:",
    "- You define `max_workers` (for example `4`) → that’s the max number of jobs running at the same time",
    "- For every job, you call `executor.submit(...)`",
    "- `submit` returns a **Future**: a handle that represents “work that will finish later”",
    "",
    "I like thinking of it as a queue of tasks where only a fixed number of workers are allowed to run simultaneously.",
    "",
    {
      "type": "image",
      "src": "machinelearning_pics/threadpool_cli_executor.png",
      "alt": "ThreadPoolExecutor usage: submit jobs, map futures to jobs, iterate with as_completed, and handle errors via future.result()"
    },
    "",
    "---",
    "### 4) Futures, `future_to_job`, and why that mapping matters",
    "One confusing part at first was this idea of mapping Futures back to jobs.",
    "",
    "Key insight:",
    "- The **Future is not the result** — it’s a *container* that will eventually hold a result (or an exception).",
    "- The mapping `future_to_job` exists so I can say: “This Future belongs to *that* input/output pair.”",
    "",
    "That becomes crucial for error handling:",
    "- `future.result()` either returns normally (job succeeded)",
    "- or it raises an exception (job failed)",
    "- and because I can look up the job from the Future, I can print *which file* failed",
    "",
    "---",
    "### 5) Why `as_completed(...)` instead of a normal loop",
    "`as_completed(futures)` yields Futures **in the order they finish**, not the order they were submitted.",
    "",
    "That matters because in real workloads:",
    "- Some files are small and finish fast",
    "- Some files are huge and finish later",
    "",
    "With `as_completed`, I get progress feedback as soon as jobs complete, and failures surface earlier instead of being hidden behind one slow file.",
    "",
    "---",
    "### 6) What I learned about concurrency (the practical version)",
    "- **Concurrency** means *overlapping work*: while one job is waiting (e.g., on disk I/O), another job can run.",
    "- A thread pool gives **controlled parallelism**: not 200 threads, but a stable number like 4 or 8.",
    "- You want to avoid shared state: each job writes to its own output file → fewer locks, fewer bugs.",
    "- Futures are the clean way to handle results and exceptions in parallel code.",
    "",
    "---",
    "### 7) Why this matters for machine learning",
    "Even before training models, ML workflows are full of “many independent tasks”:",
    "- preprocessing many files (logs, images, shards)",
    "- feature extraction per file / per chunk",
    "- dataset validation / cleaning",
    "- running evaluation across many checkpoints or parameter configs",
    "",
    "If you can express your work as **independent jobs** and process them with a controlled executor, you get:",
    "- faster iteration loops",
    "- more scalable data pipelines",
    "- and a strong foundation for later (multiprocessing, distributed systems, and MLOps tooling).",
    "",
    "---",
    "### Next steps",
    "Next I want to benchmark different thread counts (1 vs 4 vs 8) and learn how to recognize when I’m I/O-limited vs CPU-limited — because that’s the difference between “threads help a lot” and “threads don’t help at all”."
  ],
  "date": "2026-02-20"
},
  
  {
    "title": "Concurrency v2 in My CSV Toolkit: Dataclass Jobs + executor.map() (and the Tradeoffs)",
    "content": [
      "After my first concurrency version (ThreadPoolExecutor + submit() + as_completed()), I refactored the CLI into a cleaner pipeline: jobs are real objects (dataclasses), the worker returns structured results, and the pool runs everything via executor.map().",
      "This post explains what changed, why it feels “cleaner”, and what I lose compared to the original submit/as_completed approach — plus why this pattern shows up a lot in real ML/data pipelines."
    ],
    "images": [],
    "description": [
      "### What this script does",
      "The goal is still the same as my earlier CSV posts:",
      "- Find all `*.csv` files in an input directory (recursive).",
      "- Clean them **row-by-row** (streaming) and write cleaned output files.",
      "- Keep the folder structure inside the output directory.",
      "- Append `_cleaned` to the output filename.",
      "- Run multiple independent files in parallel with a limited number of threads.",
      "",
      "---",
      "### The big refactor: representing work as data (Job) and results as data (JobResult)",
      "In the first concurrency version, my job was basically just a tuple like `(input_file, output_file)`, and the rest of the configuration lived “around” the worker function (captured from `args`).",
      "In this version, I made the design more explicit:",
      "- A **Job** contains everything needed to do one unit of work (input path, output path, batch size, optional filter).",
      "- A **JobResult** contains the job plus an `ok` flag and an optional error message.",
      "",
      "This turns concurrency into a simple data flow:",
      "**build jobs → run jobs → consume results**",
      "",
      {
        "type": "image",
        "src": "machinelearning_pics/csv_concurrency_dataclass_jobs_1.png",
        "alt": "Code screenshot showing @dataclass(frozen=True) Job and JobResult, plus run_job(job) returning a JobResult with ok/error"
      },
      "",
      "---",
      "### Why `frozen=True` is a big deal (especially with threads)",
      "`@dataclass(frozen=True)` makes the object **immutable** after creation.",
      "That matters in threaded programs because it prevents accidental mutation of shared data:",
      "- No thread can “half change” a job while another thread is reading it.",
      "- The job becomes a safe payload you can pass around without thinking about locks.",
      "",
      "In practice: a frozen `Job` is like a “read-only work order”.",
      "",
      "---",
      "### The worker wrapper: `run_job(job)` returns a result instead of raising",
      "Instead of letting exceptions escape into a Future (and later calling `future.result()`), I handle errors inside the worker:",
      "- If `process_csv(...)` succeeds → return `JobResult(ok=True)`",
      "- If it fails → catch the exception and return `JobResult(ok=False, error=...)`",
      "",
      "This is a very ML/data-pipeline style pattern:",
      "- you can keep going even if one file is broken,",
      "- you can summarize failures at the end,",
      "- and you don’t have to do Future-based exception plumbing if you don’t want to.",
      "",
      "---",
      "### The concurrency part becomes extremely small: `executor.map(run_job, jobs)`",
      "The main concurrency change is switching from `submit()` + `as_completed()` to `map()`:",
      "- With `map()`, you give the pool a function and a list of jobs.",
      "- It schedules them across threads.",
      "- You iterate over returned results like a normal loop.",
      "",
      "That makes the code feel more like a pipeline and less like Future management.",
      "",
      {
        "type": "image",
        "src": "machinelearning_pics/csv_concurrency_dataclass_jobs_2.png",
        "alt": "Code screenshot showing jobs being built as Job objects and processed via ThreadPoolExecutor(...).map(run_job, jobs) with result handling"
      },
      "",
      "---",
      "### Tradeoffs vs my original `submit()` + `as_completed()` version",
      "This refactor is not “strictly better” — it’s a different style with real tradeoffs.",
      "",
      "#### ✅ What got better",
      "- **Cleaner signatures:** the pool sees a worker that takes exactly one argument (`Job`).",
      "- **Less parameter chaos:** all config is inside the job; adding fields later is easy.",
      "- **No Future bookkeeping:** no `future_to_job` dict needed — results contain `result.job` directly.",
      "- **Explicit, structured outcomes:** failures are data (`ok/error`) instead of exceptions living inside futures.",
      "",
      "#### ⚠️ What I give up",
      "- **No “finished-first” handling:** `executor.map()` yields results in *input order*, not completion order.",
      "  - If the first job is huge and slow, your result loop will appear to “wait”, even though other jobs may already be done.",
      "- **Less per-task control:** with `submit()` + futures you can more easily:",
      "  - cancel individual tasks,",
      "  - use per-future timeouts,",
      "  - process results immediately as they finish (`as_completed`).",
      "",
      "---",
      "### Why this pattern is useful in Machine Learning",
      "A lot of ML work is basically “run the same function over many independent inputs”:",
      "- preprocessing many files (CSVs, logs, parquet shards),",
      "- extracting features per file or per sample batch,",
      "- validating datasets and collecting errors,",
      "- running evaluation jobs across different subsets.",
      "",
      "The **Job + Result** pattern scales nicely because:",
      "- it’s easy to parallelize (each job is independent),",
      "- it encourages immutable inputs (`frozen=True`),",
      "- it makes failures trackable without killing the whole run.",
      "",
      "---",
      "### What I learned",
      "- Concurrency becomes easier when you design the work as **data** first (jobs).",
      "- A “single-argument worker” is a powerful mental model for thread pools.",
      "- Returning structured results is often cleaner than relying on Future exceptions.",
      "- `map()` is great for pipeline-style code; `as_completed()` is better when you care about *completion order* and per-task control."
    ],
    "date": "2026-02-21"
  },
{
  "title": "Producer–Consumer Pipeline in Python: Queue, Sentinel, Event & Locks (and when it’s worth it)",
  "content": [
    "After I built my CSV cleaner as a clean streaming pipeline using generators, I moved on to the next learning step: a Producer–Consumer pipeline using `queue.Queue` (Reader → Worker → Writer). The goal wasn’t “maximum speed at all costs”, but understanding how threads communicate safely, how to handle cancellation/errors robustly, and why a correct end-of-stream signal (a sentinel) is critical.",
    "This post summarizes what I learned—based on the exact questions I asked along the way: Why `put()` instead of `append()`? Who wakes whom? Why multiple sentinels? What are `Event` and `Lock` for? Why use `cast()`? And when is this pattern actually useful compared to my previous generator-based `process_csv` implementation without pipeline threads?"
  ],
  "images": [],
  "description": [
    "### Overview: What I built",
    "I implemented a pipeline version of `process_csv(...)` that consists of three roles:",
    "- **Reader**: reads CSV rows and puts them into a queue",
    "- **Worker(s)**: clean/filter rows in parallel",
    "- **Writer**: writes cleaned rows to the output file in batches",
    "",
    {
      "type": "image",
      "src": "pipeline-setup-queues-event-locks.png",
      "alt": "process_csv_pipeline signature: parameters, sentinel = object(), in_q/out_q as Queue(maxsize=...), stop_event, locks, first_exc and stats"
    },
    "",
    "---",
    "### Why `Event` and `Lock` at all",
    "One key realization: not everything you “call with parentheses” is a function.",
    "- `threading.Event()` creates an **Event object** (a shared on/off switch for threads).",
    "- `threading.Lock()` returns a **Lock object** (a mutex so two threads don’t mutate shared state at the same time).",
    "",
    "In my pipeline, I use:",
    "- an `Event` (`stop_event`) to signal “stop early” across threads (e.g., after the first error).",
    "- `Lock`s to protect shared state (`first_exc` and `stats`) from concurrent writes.",
    "",
    "---",
    "### `Queue.put()` instead of `append()` — what’s the core idea?",
    "My biggest takeaway: a `Queue` is not “just a list”. It’s a thread-friendly buffer that automatically provides:",
    "1) **Thread safety**: multiple threads can call `put()`/`get()` without me adding my own locks.",
    "2) **Backpressure**: if `maxsize` is reached, `put()` waits automatically instead of letting memory grow without bounds.",
    "3) **No busy waiting**: if the queue is empty, `get()` waits automatically, so workers don’t spin in a loop checking repeatedly.",
    "",
    "This also answers my question about “communication”: the queue doesn’t store “requests” like a log; it maintains internal wait-lists for threads that are currently blocked.",
    "",
    "---",
    "### Who wakes whom? (the “bell” happens inside `Queue`, not in my code)",
    "This behavior is automatic inside the queue implementation:",
    "- **A consumer gets woken up** when a producer successfully puts an item in:",
    "  - conceptually: “queue is no longer empty”",
    "  - technically (simplified): `not_empty.notify()`",
    "- **A producer gets woken up** when a consumer successfully removes an item:",
    "  - conceptually: “queue is no longer full”",
    "  - technically (simplified): `not_full.notify()`",
    "",
    "Important term from my debugging: **“blocked”** means `put()`/`get()` is waiting—and yes, the program can “freeze” if nobody drains the queue anymore.",
    "",
    "---",
    "### Sentinel: Why `sentinel = object()` and why as many as there are workers?",
    "At first I thought: “Isn’t a single sentinel enough?”",
    "But: if there is only **one** sentinel, only **one** worker will see it and exit—the others will wait forever in `get()`.",
    "",
    "That’s why:",
    "- The reader inserts **exactly `worker_count` sentinels** into `in_q` (so every worker gets one).",
    "- Each worker that sees a sentinel inserts **one sentinel** into `out_q` and exits.",
    "- The writer counts sentinels from `out_q` and stops only after seeing **`worker_count`** of them.",
    "",
    "That way the writer knows for sure: “All workers are really done; nothing else will arrive.”",
    "",
    {
      "type": "image",
      "src": "reader-thread-finally-sentinels.png",
      "alt": "record_exc() guarded by exc_lock and reader_thread(): reads read_rows(), put(row), stop_event check, and finally: in_q.put(sentinel) for each worker"
    },
    "",
    "---",
    "### Why `finally` in the reader is so important",
    "`finally` always runs—even if an error happens or I abort early.",
    "This prevents hangs:",
    "- If the reader doesn’t send sentinels, workers can wait forever.",
    "- If workers stop consuming too early, the reader can block trying to insert sentinels.",
    "",
    "In these pipelines, a correct end signal isn’t a nice-to-have—it’s required.",
    "",
    "---",
    "### `stop_event`: Why workers don’t instantly `break` when stop is set",
    "My naive assumption was: “Stop is set → the worker should immediately exit.”",
    "But that can cause deadlocks if, for example, the queue is full and the reader still needs to insert sentinels in its `finally` block.",
    "",
    "So when stop is set, a worker behaves like this:",
    "- **no more real work** (no cleaning, no `out_q.put(cleaned)`)",
    "- but it keeps calling `in_q.get()` to drain until it sees a sentinel",
    "",
    "This keeps queues drainable and makes shutdown deterministic (even on errors).",
    "",
    {
      "type": "image",
      "src": "worker-thread-stop-event-cast-sentinel.png",
      "alt": "worker_thread(): in_q.get(), sentinel handling, stop_event.is_set(): continue, cast(Dict[str,str], item), clean_row(), out_q.put(cleaned), exception handling with out_q.put_nowait(sentinel)"
    },
    "",
    "---",
    "### Why `out_q.put_nowait(sentinel)` on errors (and what `pass` means)",
    "I learned that “best effort” means: *try to avoid the writer hanging—without hanging yourself*.",
    "If `out_q` is full, `out_q.put(sentinel)` can block, which is bad during error shutdown.",
    "",
    "So `put_nowait(...)` is safer: if there is no space it raises `Full`, and I can ignore that.",
    "",
    "`pass` in Python means “do nothing”. Here it means: don’t let a secondary problem (queue full) hide the original error.",
    "",
    "---",
    "### Writer: why I still use a `batch`, but not the `batcher()` generator",
    "In the pipeline I implemented batching directly in the writer:",
    "- accumulate rows in `batch`",
    "- once it reaches `batch_size`: `writer.writerows(batch)` and `batch.clear()`",
    "",
    "I don’t use `batcher(...)` because the writer also needs to count sentinels and flush cleanly. A single loop is easier to read.",
    "",
    {
      "type": "image",
      "src": "writer-thread-batching-sentinels.png",
      "alt": "writer_thread(): out_q.get(), count sentinels, write header on first row, collect rows into batch and write in batch_size chunks"
    },
    "",
    "---",
    "### `cast(Dict[str, str], item)` — does it actually validate anything?",
    "No: `cast(...)` performs **no runtime checks**.",
    "It’s only a hint for IDE/type checkers:",
    "- “From here on, treat `item` as Dict[str, str]”",
    "",
    "Why do I need it?",
    "- Because I type the queue as `Queue[object]` (due to the sentinel).",
    "- Without `cast`, the type checker can’t know that after the sentinel check, `item` is in fact a dict row.",
    "",
    "If I wanted a runtime check, I’d use `isinstance(item, dict)`. But in this design, `item` is either sentinel or row dict.",
    "",
    "---",
    "### Starting and waiting on threads: why `start()` and `join()` are ordered this way",
    "`start()` launches a thread. `join()` only means “wait until it finishes”.",
    "A thread finishes by itself when its function returns—`join()` doesn’t terminate it.",
    "",
    "I start in this order:",
    "- writer first (so output can drain immediately)",
    "- then workers",
    "- then reader",
    "",
    "And I wait in this order:",
    "- reader first (so we know no new input will arrive)",
    "- then workers (so they all finish/stop)",
    "- then writer (so it flushes and exits cleanly)",
    "",
    {
      "type": "image",
      "src": "thread-start-join-order-and-error-propagation.png",
      "alt": "Thread creation (reader/writer/workers), start order, join order, error re-raise via first_exc and final stats print"
    },
    "",
    "---",
    "### When is which kind of parallelism worth it?",
    "I built a practical rule-of-thumb for myself:",
    "",
    "**Many files (small/medium/large):**",
    "- prefer **parallelism across files** (like my `cli.py` using ThreadPoolExecutor),",
    "- but keep thread count moderate (e.g. **2–8**, benchmark).",
    "",
    "**One / a few huge file(s):**",
    "- if each row involves **I/O waiting** (API/DB): **pipeline/async is worth it**",
    "- if each row is **CPU heavy**: prefer a **ProcessPool**",
    "- if each row is very light (my cleaner): usually just **stream**, maybe only mild parallelism",
    "",
    "**Important:** Don’t max out both levels (file-parallel AND row-parallel) unless you consciously limit total threads.",
    "",
    "---",
    "### Important: “stacking” threads (pipeline + CLI threads)",
    "If I use a per-file pipeline while also processing many files in parallel, the thread count multiplies:",
    "- per file ≈ `worker_count + 2` threads (Reader + Writer + Workers)",
    "- times `--threads` in the CLI",
    "",
    "Example: `--threads 8` and `worker_count 4` → roughly `8 * (4 + 2) = 48` threads.",
    "",
    "In practice it’s usually either:",
    "- file-parallel (CLI threads higher), pipeline off",
    "- or pipeline on (higher worker_count), CLI threads low (often 1–2)",
    "",
    "---",
    "### Tradeoffs vs. my previous `process_csv(...)` without pipeline threads",
    "What improved:",
    "- clear separation of roles (Reader/Worker/Writer)",
    "- backpressure via `Queue(maxsize=...)`",
    "- more robust cancellation/error handling (sentinel + stop_event + first_exc)",
    "",
    "What got worse / more expensive:",
    "- more code complexity",
    "- more overhead (queue `put/get`, thread scheduling)",
    "- for a very light cleaner, this can even be slower than the simple generator chain",
    "",
    "Bottom line: a pipeline is a great pattern, but it isn’t automatically the fastest solution for every job.",
    "",
    "---",
    "### Time and space complexity (Big-O, rough)",
    "**Time:**",
    "- For `N` input rows, the core work is **O(N)**.",
    "- Each row is read, (maybe) cleaned, and (maybe) written.",
    "- Parallelism can reduce wall-clock time if there is I/O waiting, but Big-O remains O(N).",
    "",
    "**Space:**",
    "- The pipeline does not materialize the entire file, but it does buffer a bounded amount:",
    "  - `in_q` up to `queue_size` items",
    "  - `out_q` up to `queue_size` items",
    "  - writer `batch` up to `batch_size` items",
    "",
    "So roughly: **O(queue_size + batch_size)** (times the size of one row).",
    "",
    "The generator `read_rows()` stays effective (no full materialization), but queues are a deliberate, bounded “staging area”."
  ],
  "date": "2026-02-22"
}
]
