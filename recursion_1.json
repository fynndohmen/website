 [
  {
  "title": "Recursion – Fundamentals, When to Use It, Pros and Cons",
  "content": [
    "Recursion is a technique where a function calls itself to solve smaller instances of the same problem.",
    "This post explains what recursion is, how it works, when it is useful, and how it affects time and space complexity."
  ],
  "images": [],
  "description": [
    "## What is recursion?",
    "Recursion is a way of defining a function, algorithm or data structure in terms of **smaller versions of itself**.",
    "",
    "In code, a recursive function is a function that calls itself:",
    "- On a **smaller subproblem** (for example `f(n-1)` instead of `f(n)`),",
    "- Until it reaches a **base case** that can be solved directly without further recursion.",
    "",
    "A typical recursive pattern:",
    "- **Base case:** handles the smallest, simplest input directly (for example `n == 0`).",
    "- **Recursive case:** reduces the problem to one or more smaller subproblems and calls the same function on them.",
    "",
    "Without a correct base case or with a wrong reduction step, recursion can lead to infinite loops or stack overflows.",
    "",
    "## Direct vs indirect recursion",
    "There are two common forms:",
    "",
    "- **Direct recursion:** a function directly calls itself.",
    "  - Example: `factorial(n)` calls `factorial(n - 1)`. ",
    "",
    "- **Indirect recursion:** a function calls another function, which eventually calls the first function again.",
    "  - Example: `even(n)` calls `odd(n-1)` and `odd(n)` calls `even(n-1)`. ",
    "",
    "Most algorithmic problems use **direct recursion**, but complex systems or grammars sometimes use indirect recursion.",
    "",
    "## Typical use cases",
    "Recursion is especially natural when the problem itself has a **recursive structure**:",
    "",
    "- **Tree and graph traversals:**",
    "  - Preorder, inorder, postorder traversal of binary trees.",
    "  - Depth-first search (DFS) on graphs.",
    "",
    "- **Divide-and-conquer algorithms:**",
    "  - Merge sort, quicksort, binary search.",
    "  - Algorithms that split a problem into halves or into subranges.",
    "",
    "- **Backtracking and search:**",
    "  - Solving mazes, Sudoku, N-Queens, subset search.",
    "  - Exploring all combinations or permutations under constraints.",
    "",
    "- **Mathematical recurrences and definitions:**",
    "  - Fibonacci numbers, factorial, Catalan numbers.",
    "  - Many combinatorial counting problems.",
    "",
    "## Advantages of recursion",
    "- **Natural expression of recursive structure:**",
    "  - Tree-like or divide-and-conquer problems often mirror their own definition when written recursively.",
    "",
    "- **Cleaner and shorter code:**",
    "  - Recursive implementations can be much easier to read than equivalent iterative versions, especially for trees and backtracking.",
    "",
    "- **Direct mapping from mathematical recurrence to code:**",
    "  - If you already have a recurrence relation, recursive code often follows it almost line by line.",
    "",
    "- **Good for exploration algorithms:**",
    "  - Backtracking and DFS are often clearer and less error-prone when written recursively.",
    "",
    "## Disadvantages of recursion",
    "- **Call stack overhead:**",
    "  - Each recursive call consumes stack space. Deep recursion can lead to **stack overflow** if the recursion depth is large.",
    "",
    "- **Potentially worse performance than iterative solutions:**",
    "  - Function call overhead and repeated argument passing can be slower than a simple loop.",
    "",
    "- **Harder to reason about for beginners:**",
    "  - Understanding the flow of control and unwinding of recursive calls can be tricky.",
    "",
    "- **Risk of exponential blow-up:**",
    "  - A naive recursive solution (for example naïve Fibonacci) can have **exponential time complexity** if it recomputes the same subproblems many times.",
    "",
    "## Recursion and the call stack",
    "Every recursive call creates a new **stack frame** that stores:",
    "- The function's parameters,",
    "- Local variables,",
    "- The return address (where to continue after the function returns).",
    "",
    "The **recursion depth** (the maximum number of active recursive calls at once) directly affects space usage:",
    "- If the maximum depth is `d`, the call stack typically uses **O(d)** extra space.",
    "- For example:",
    "  - Recursive DFS on a tree of height `h` uses O(h) stack space.",
    "  - Recursive binary search uses O(log n) stack space on an array of size n.",
    "",
    "If `d` becomes too large (for example recursion on a linked list with millions of elements), you can hit a stack overflow and should consider rewriting the algorithm iteratively.",
    "",
    "## Time and space complexity",
    "Recursion itself does not magically change complexity — it is just a different way to **organize the same operations**. The complexity comes from:",
    "",
    "- **How many recursive calls** you make overall.",
    "- **How much work** you do in each call.",
    "",
    "You can often describe a recursive algorithm with a recurrence relation:",
    "- Example (binary search): `T(n) = T(n/2) + O(1)` → `T(n) = O(log n)`.",
    "- Example (merge sort): `T(n) = 2 * T(n/2) + O(n)` → `T(n) = O(n log n)`.",
    "",
    "Space complexity:",
    "- **Call stack:** O(depth) extra space.",
    "- Plus any additional data structures the algorithm uses.",
    "",
    "In some cases, you can convert a recursive algorithm to an iterative one that:",
    "- Keeps the same time complexity,",
    "- But reduces stack usage from O(depth) to O(1), or uses an explicit stack that you control.",
    "",
    "## Recursion vs iteration",
    "- **Iteration (loops):**",
    "  - Uses constructs like `for` and `while`.",
    "  - State is kept in variables; no additional call stack frames are created.",
    "",
    "- **Recursion:**",
    "  - Uses function calls to move forward through the problem.",
    "  - State for each step is stored in the call stack.",
    "",
    "In theory, any recursive algorithm can be transformed into an iterative one (using an explicit stack or other data structures). In practice:",
    "- Choose **recursion** when it makes the algorithm much easier to understand (trees, backtracking, divide-and-conquer).",
    "- Choose **iteration** when you need tight control over performance and stack usage, or when the recursion depth would be too large.",
    "",
    "## Summary",
    "Recursion is a powerful way to define algorithms in terms of smaller versions of the same problem. It shines for tree-structured data, divide-and-conquer algorithms and backtracking, often resulting in short and elegant code. However, recursion uses the call stack, which costs extra space and can cause stack overflows at large depths. Understanding how recursion maps to the call stack and to recurrence relations is key to writing correct and efficient recursive algorithms — and to knowing when an iterative approach or dynamic programming might be a better choice."
  ],
  "date": "2025-12-10",
  "pinned": true
}
,

  {
    "title": "Calculating the Nth Fibonacci Number Using Recursion",
    
    "content": [
        "This Python script demonstrates how to calculate the Nth Fibonacci number using a recursive approach."
    ],
    
    "images": [
        "algorithms_pics/recursive_fibonacci.png"
    ],
    
    "description": [
        "### Key Features of the Script:",
        
        "1. **Fibonacci Sequence:**",
        "   - The Fibonacci sequence starts with 0 and 1, and each subsequent number is the sum of the previous two numbers.",
        
        "2. **Recursive Approach:**",
        "   - The function calls itself recursively to compute the Fibonacci number at position `n`.",
        "   - The base cases are defined: `gertNthFib(1) = 0` and `gertNthFib(2) = 1`.",
        "   - For `n > 2`, the function returns `gertNthFib(n-1) + gertNthFib(n-2)`, simulating the Fibonacci sequence.",
        
        "3. **Output:**",
        "   - The function returns the Nth Fibonacci number as specified by the input.",
        
        "### Complexity Analysis:",
        
        "**Time Complexity:**",
        "- Each call to `gertNthFib(n)` results in two additional recursive calls until the base case is reached.",
        "- This results in an **exponential time complexity of O(2^n)**, making it inefficient for large values of `n`.",
        
        "**Space Complexity:**",
        "- The recursive approach uses a call stack proportional to `n`, leading to a space complexity of **O(n)** in the worst case.",
        "- Since no additional data structures are used, only recursive stack memory is required."
    ],
    
    "date": "2025-01-07"
},
  {
    "title": "Finding the Nth Fibonacci Number Using Iteration",
    
    "content": [
        "This Python script calculates the Nth Fibonacci number using an iterative approach to ensure efficiency and simplicity."
    ],
    
    "images": [
        "algorithms_pics/nth_fibonacci.png"
    ],
    
    "description": [
        "### Key Features of the Script:",
        
        "1. **Fibonacci Sequence:**",
        "   - The Fibonacci sequence starts with 0 and 1, and each subsequent number is the sum of the previous two numbers.",
        
        "2. **Iterative Calculation:**",
        "   - Instead of using recursion, the script uses a loop to calculate the sequence iteratively.",
        "   - This approach avoids excessive memory usage or stack overflow issues.",
        
        "3. **Logic Flow:**",
        "   - For the first Fibonacci number (`n = 1`), it returns 0.",
        "   - For the second Fibonacci number (`n = 2`), it returns 1.",
        "   - For `n > 2`, the script calculates the sequence by summing the last two numbers until the Nth number is reached.",
        
        "4. **Output:**",
        "   - The function returns the Nth Fibonacci number as specified by the input.",
        
        "### Complexity Analysis:",
        
        "**Time Complexity:**",
        "- The function iterates through the sequence using a single loop.",
        "- Since the loop runs `n-2` times, the overall time complexity is **O(n)**.",
        
        "**Space Complexity:**",
        "- The function only uses two integer variables (`prev` and `curr`).",
        "- Since no additional lists or recursive calls are used, the space complexity is **O(1)**."
    ],
    
    "date": "2025-01-08"
}
,
{
    "title": "Calculating Product Sums of Nested Arrays in Python",
    
    "content": [
        "This Python script computes the product sum of a nested array. The product sum is calculated by summing all elements of the array, with elements inside nested arrays being multiplied by their depth."
    ],
    
    "images": [
        "algorithms_pics/product_sum.png"
    ],
    
    "description": [
        "### Key Features of the Product Sum Script:",
        
        "1. **Recursive Approach:**",
        "   - The function `productSum()` uses recursion to handle nested arrays. When an element is a list, the function calls itself with an incremented depth.",
        
        "2. **Depth Multiplier:**",
        "   - For every level of nesting, the sum of elements is multiplied by the current depth. This adds weight to elements that are deeper in the array structure.",
        
        "3. **Handling Mixed Structures:**",
        "   - The function can handle arrays with a mix of integers and nested arrays, processing them seamlessly.",
        
        "### How It Works:",
        
        "1. The script initializes the `result` to 0 and iterates over the elements of the input array.",
        "2. If an element is a list, the function recursively calculates its product sum with an incremented depth.",
        "3. If an element is an integer, it is added directly to the `result`.",
        "4. After iterating through the array, the cumulative sum is multiplied by the current depth and returned.",
        
        "### Complexity Analysis:",
        
        "**Time Complexity:**",
        "- The function processes every element (integers + nested lists) exactly once.",
        "- Since every element contributes to the total sum, the time complexity is **O(E)**, where `E` is the total number of elements, including both integers and nested lists.",
        
        "**Space Complexity:**",
        "- The recursive calls require a stack proportional to the depth `d` of the nested list.",
        "- In the worst case, where the nesting depth is maximum (e.g., a deeply nested list), the space complexity is **O(d)**."
    ],
    
    "date": "2025-01-09"
},
  {
"title": "Generating All Permutations (Recursive Build with Slicing)",
"content": [
"This Python routine produces every permutation of an input array using a clean recursive strategy that builds the current permutation while shrinking the remaining pool of elements."
],
"images": ["algorithms_pics/permutations_recursive.png"],
"description": [
"### Problem Statement:",
"- Given an array of distinct (or even non-distinct) values, generate **all permutations**.",
"- Return them as a list of lists, where each inner list is one ordering of the original elements.",
"",
"---",
"### How the Algorithm Works:",
"1. **Driver:** `getPermutations(array)` initializes a result list and calls `permutationsHelper(array, [], permutations)`.",
"",
"2. **State Representation:**",
"   - `array`: the pool of elements **not yet used**.",
"   - `currentPermutation`: the **partial permutation** being built.",
"   - `permutations`: the **output collector**.",
"",
"3. **Base Case:**",
"   - `if not len(array) and len(currentPermutation):`",
"   - When there’s nothing left to choose (`array` empty), the current build is a **complete permutation** → append it.",
"",
"4. **Recursive Step:**",
"   - Iterate `i` through the remaining elements.",
"   - Choose `array[i]`, form:",
"     - `newArray = array[:i] + array[i + 1:]` (all remaining except the chosen one),",
"     - `newPermutation = currentPermutation + [array[i]]` (extend the path).",
"   - Recurse on `(newArray, newPermutation)`; this explores the subtree where `array[i]` occupies the **next position**.",
"",
"---",
"### Why It Enumerates Everything Exactly Once:",
"- Each level fixes one more position in the permutation.",
"- Slicing removes the chosen item, so deeper calls **cannot reuse** it.",
"- The tree has branching factor equal to the remaining items count: `n, n-1, ..., 1` → exactly `n!` leaves.",
"",
"---",
"### Complexity:",
"- **Output size:** There are `n!` permutations; any correct algorithm must output them all.",
"- **Time (rough):** `O(n · n!)` to visit leaves and build each permutation of length `n`.",
"- **Time (with slicing overhead):** Due to `array[:i] + array[i+1:]` creating a fresh list at each step, the **upper bound** is `O(n^2 · n!)`.",
"- **Space:** `O(n · n!)` for storing results; auxiliary recursion depth is `O(n)` plus temporary lists from slicing."
],
"date": "2025-11-10"
},
  {
"title": "In-Place Permutations via Backtracking (Swap-Based DFS)",
"content": [
"This Python implementation generates all permutations of an array **in place** by fixing positions left-to-right and exploring choices via swaps, then undoing each swap (backtracking)."
],
"images": ["algorithms_pics/permutations_inplace.png"],
"description": [
"### What the Code Does",
"- `getPermutations(array)` returns a list of all permutations.",
"- `permutationsHelper(i, array, permutations)` fixes the element at index `i` and recursively permutes the suffix.",
"- `swap(array, i, j)` exchanges two positions so we can try a choice without extra arrays."
,
"---",
"### How It Works (Backtracking)",
"1. **Fix prefix:** At depth `i`, the first `i` elements are already chosen.",
"2. **Enumerate choices:** For `j` from `i` to `len(array)-1`, swap `array[i]` with `array[j]`. Now the element at `i` is the next choice.",
"3. **Recurse:** Call `permutationsHelper(i+1, ...)` to permute the remaining suffix.",
"4. **Backtrack:** Swap back (`swap(array, i, j)`) to restore the array before trying the next `j`.",
"5. **Leaf:** When `i == len(array)-1`, we’ve fixed all positions; append a copy `array[:]` to results."
,
"---",
"### Tiny Walkthrough (array = [1,2,3])",
"- At `i=0`, try `j=0` (no-op), recurse to `i=1`: produce `[1,2,3]`, `[1,3,2]`.",
"- Backtrack to `i=0`, try `j=1` (swap → `[2,1,3]`), recurse: produce `[2,1,3]`, `[2,3,1]`.",
"- Backtrack to `i=0`, try `j=2` (swap → `[3,2,1]`), recurse: produce `[3,2,1]`, `[3,1,2]`.",
"- All `3!` permutations are explored systematically."
,
"---",
"### Why It’s Correct",
"- **Completeness:** For each position `i`, the loop tries **every** remaining element at that position; recursion covers all suffix orders.",
"- **No duplicates from position reuse:** Swapping fixes one element per depth; backtracking restores state so future choices start from the same clean prefix.",
"- **State restoration:** The second swap undoes the first, keeping the array valid for the next branch."
,
"---",
"### Complexity",
"- **Time:** `O(n · n!)` — there are `n!` leaves and copying each permutation costs `O(n)`.",
"- **Space:** `O(n)` auxiliary for recursion depth (the array is reused in place). Result storage is `O(n · n!)` by definition."
,
"---",
"### Practical Notes",
"- Works with any comparable elements.",
"- This swap-based approach avoids allocating new subarrays at each step, making it faster and more memory-friendly than slice-and-build methods."
],
"date": "2025-11-11"
}



]
