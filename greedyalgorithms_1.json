[
 {
  "title": "Greedy Algorithms – Fundamentals, When They Work, Pros and Cons",
  "content": [
    "Greedy algorithms build a solution step by step by always making the locally optimal choice at each step.",
    "This post explains what greedy algorithms are, when they work, where they fail, and how they compare to dynamic programming and brute-force approaches."
  ],
  "images": [],
  "description": [
    "## What is a greedy algorithm?",
    "A greedy algorithm is a strategy that builds a solution **piece by piece**, always choosing the option that looks best **right now** (locally optimal) without reconsidering earlier decisions.",
    "",
    "The key idea:",
    "- At each step, pick the choice that seems best according to some **greedy rule** (for example smallest weight, earliest finish time, largest value-to-weight ratio).",
    "- Do **not** backtrack or explore alternative choices.",
    "- Hope (and prove) that this leads to a **globally optimal** solution for the problem.",
    "",
    "Greedy algorithms are attractive because they are usually **simple and fast**, but they only work reliably for problems with certain structural properties.",
    "",
    "## When is a greedy algorithm applicable?",
    "Greedy algorithms tend to work well when the problem has special properties such as:",
    "",
    "- **Greedy-choice property:** a globally optimal solution can be constructed by making a locally optimal choice at each step.",
    "- **Optimal substructure:** an optimal solution to the problem contains optimal solutions to its subproblems.",
    "",
    "Not every problem with optimal substructure supports a correct greedy algorithm (many require dynamic programming instead), but:",
    "- If you can prove that a certain greedy rule **always leads to an optimal solution**, the resulting algorithm is often much simpler and faster than a DP or brute-force solution.",
    "",
    "## Typical use cases",
    "- **Interval scheduling / activity selection:** choose the maximum number of non-overlapping intervals by always picking the one that finishes earliest.",
    "- **Minimum spanning tree:** Kruskal’s and Prim’s algorithms are greedy – they repeatedly choose the cheapest edge that keeps the partial structure valid.",
    "- **Shortest paths with non-negative weights:** Dijkstra’s algorithm greedily picks the unvisited node with the smallest distance so far.",
    "- **Huffman coding:** repeatedly merge the two least frequent symbols to build an optimal prefix code.",
    "- **Fractional knapsack:** take items in order of decreasing value-to-weight ratio, possibly taking fractions of items.",
    "- **Scheduling with deadlines and penalties/profits:** many variants admit greedy solutions (for example sort by deadline or profit per time).",
    "",
    "## Advantages of greedy algorithms",
    "- **Simple to design and implement:** once you know the greedy rule, the algorithm is usually short and readable.",
    "- **Fast in practice:**",
    "  - Often dominated by a sort step → O(n log n) time.",
    "  - Or by operations on a priority queue / heap → O(m log n) in graph problems.",
    "- **Low memory usage:**",
    "  - Typically only store the current partial solution and some small auxiliary data structures → O(n) or better.",
    "- **Great for large inputs:** when they apply, their combination of speed and low memory usage makes them very practical.",
    "",
    "## Disadvantages of greedy algorithms",
    "- **Not always correct:**",
    "  - For many optimization problems (for example 0/1 knapsack, general scheduling, many path problems), the most obvious greedy rule produces **suboptimal** solutions.",
    "- **Hard to know when they work:**",
    "  - You usually need a **mathematical proof** (exchange argument, cut property, etc.) to be sure a greedy algorithm is correct.",
    "- **No backtracking:**",
    "  - Once a local decision is made, the algorithm does not revisit it. This is what makes it fast, but also what makes it risky for problems without the right structure.",
    "",
    "## Greedy vs Dynamic Programming and brute force",
    "- **Greedy vs brute force:**",
    "  - Brute force explores many or all possible solutions → often exponential time.",
    "  - A successful greedy algorithm jumps straight to a good solution, typically in polynomial time (often O(n log n)).",
    "",
    "- **Greedy vs dynamic programming (DP):**",
    "  - DP systematically considers all relevant subproblems and stores their results → often guarantees optimality but can be slower and use more memory.",
    "  - Greedy uses a simple local rule and does not consider all subproblems → much faster and lighter when it is correct.",
    "  - Many classic DP problems (for example 0/1 knapsack, edit distance) **do not** admit a correct greedy solution.",
    "",
    "## How greedy algorithms affect time and space complexity",
    "Greedy algorithms do not have a single universal complexity; instead, typical patterns look like this:",
    "",
    "- **Sorting-based greedy:**",
    "  - Many greedy strategies start by sorting items by some key (for example finish time, weight, ratio).",
    "  - Time complexity is dominated by the sort → **O(n log n)**.",
    "  - After sorting, a single pass is usually O(n).",
    "",
    "- **Priority-queue-based greedy:**",
    "  - Algorithms like Dijkstra, Prim or some scheduling algorithms maintain a set of candidates in a **heap / priority queue**.",
    "  - Each push/pop is O(log n); if you do this O(m) times (for example for each edge), the total time is often **O(m log n)**.",
    "",
    "Space complexity is usually:",
    "- **O(n)** for storing the input plus some extra arrays/priority queues.",
    "- Often better than dynamic programming, which may require O(n^2) or more space for large tables.",
    "",
    "## How to think about designing a greedy algorithm",
    "When approaching a new problem, a typical workflow is:",
    "",
    "1. **Start with a simple greedy idea:**",
    "   - For example \"always take the smallest\", \"always take the earliest finishing\", \"always take the most profitable per unit\".",
    "2. **Try small examples by hand:**",
    "   - Look for counterexamples where your greedy rule fails.",
    "3. **If it seems to work, try to prove it:**",
    "   - Use an *exchange argument* (show that any optimal solution can be transformed into one that makes the same greedy choice) or other structural arguments.",
    "4. **If you find a counterexample:**",
    "   - Either refine the greedy rule, or switch to dynamic programming or another technique.",
    "",
    "## Summary",
    "Greedy algorithms solve problems by repeatedly making locally optimal choices without revisiting past decisions. When a problem has the right structural properties (greedy-choice property and optimal substructure), this leads to very simple, fast and memory-efficient algorithms like interval scheduling, Dijkstra, Kruskal, Prim and Huffman coding. However, many optimization problems do **not** admit a correct greedy solution, so it is crucial to understand the problem structure and, ideally, prove that the greedy strategy always yields an optimal result."
  ],
  "date": "2025-12-10",
  "pinned": true
}
,

 {
    "title": "Calculating Minimum Waiting Time for Queries",
    "content": [
        "This Python script calculates the minimum total waiting time for a series of tasks or queries when they are executed in sequence."
    ],
    "images": ["algorithms_pics/minimum_waiting_time.png"],
    "description": [
        "### Key Features of the Minimum Waiting Time Script:",
        "1. **Sorting for Optimization:**",
        "   - The function begins by sorting the query durations in ascending order. This ensures that shorter queries are processed first, minimizing the waiting time for longer queries.",
        "",
        "2. **Logic for Calculating Waiting Time:**",
        "   - A cumulative sum (`cumulative_duration`) is maintained to keep track of the total duration of queries that have been processed so far.",
        "   - For each query, the cumulative duration is added to the total waiting time before being updated with the current query's duration.",
        "",
        "3. **Output:**",
        "   - The function returns the minimum total waiting time for all queries.",
        "",
        "### Complexity Analysis:",
        "**Time Complexity:**",
        "- The function sorts the query list, which takes **O(n log n)** time due to Timsort.",
        "- After sorting, the function iterates through the list once, requiring **O(n)** time.",
        "- Since sorting dominates, the overall time complexity is **O(n log n)**.",
        "",
        "**Space Complexity:**",
        "- The sorting is done in-place, requiring no additional space beyond the input list.",
        "- Only a few integer variables (`total_waiting_time`, `cumulative_duration`) are used, resulting in constant **O(1)** space usage.",
        "- Therefore, the overall space complexity is **O(1)**.",
        "",
        "### Why This is a Greedy Algorithm:",
        "- The algorithm makes **locally optimal choices** at each step by always processing the shortest query first.",
        "- Sorting the queries in ascending order ensures that each task contributes the least possible waiting time to subsequent tasks.",
        "- Since each decision (processing the shortest query first) leads to the **globally optimal solution** (minimum total waiting time), this follows the greedy algorithm paradigm.",
        "- Greedy algorithms work well for this problem because **once a shorter task is processed, it cannot be rearranged to achieve a better result**."
    ],
    "date": "2025-01-04"
},
{
    "title": "Maximizing Speed with the Tandem Bicycle Problem",
    "content": [
        "This Python script calculates the maximum or minimum possible total speed of pairs of cyclists on a tandem bicycle."
    ],
    "images": ["algorithms_pics/tandem_bicycle.png"],
    "description": [
        "### Key Features of the Tandem Bicycle Script:",
        "1. **Input and Sorting:**",
        "   - The function takes two lists of cyclist speeds, `redShirtSpeeds` and `blueShirtSpeeds`, and a boolean `fastest` to determine whether the goal is to maximize or minimize total speed.",
        "   - Both speed lists are sorted in ascending order. If `fastest` is `True`, the `blueShirtSpeeds` list is reversed into descending order to pair the fastest cyclist with the slowest one for maximum speed.",
        "",
        "2. **Pairing Cyclists:**",
        "   - The script iterates through the sorted lists, pairing cyclists from `redShirtSpeeds` and `blueShirtSpeeds`.",
        "   - For each pair, the higher speed is selected to maximize the total speed of the tandem bicycle.",
        "",
        "3. **Output:**",
        "   - The function returns the total speed of the paired cyclists based on the pairing strategy determined by the `fastest` parameter.",
        "",
        "### Complexity Analysis:",
        "**Time Complexity:**",
        "- The function sorts both `redShirtSpeeds` and `blueShirtSpeeds`, each requiring **O(n log n)** time due to Timsort.",
        "- If `fastest` is `True`, reversing `blueShirtSpeeds` requires **O(n)** time.",
        "- The iteration through both lists to compute the total speed requires **O(n)** time.",
        "- Since sorting dominates, the overall time complexity is **O(n log n)**.",
        "",
        "**Space Complexity:**",
        "- Sorting is done in-place, requiring no additional memory beyond the input lists.",
        "- Only a few integer variables (`totalSpeed`, loop indices) are used, leading to a constant **O(1)** space usage.",
        "- Therefore, the overall space complexity is **O(1)**.",
        "",
        "### Why This is a Greedy Algorithm:",
        "- The algorithm makes **locally optimal choices** at each step by always selecting the fastest available cyclist for the tandem.",
        "- By sorting and optionally reversing `blueShirtSpeeds`, it ensures that each pair contributes the maximum possible speed.",
        "- Greedy algorithms work well here because **once a pairing decision is made, it cannot be improved by reordering**.",
        "- This guarantees that the final solution is the globally optimal one for maximizing or minimizing the total tandem speed."
    ],
    "date": "2025-01-05"
},
    {
"title": "Optimal Task Pairing to Minimize Total Time (Greedy with Index Buckets)",
"content": [
"This Python solution pairs tasks for k workers so that the total time per worker is balanced. It sorts task durations and always pairs the shortest with the longest, returning the original indices of the paired tasks."
],
"images": ["algorithms_pics/task_assignment.png"],
"description": [
"### Problem Statement:",
"- You’re given a list tasks of positive integers (durations) and an integer k where len(tasks) = 2k.",
"- Assign exactly two tasks to each of the k workers.",
"- Goal: pair tasks so that the maximum total time assigned to any worker is as small as possible.",
"",
"---",
"### Key Idea (Greedy Optimality):",
"- Sort the durations.",
"- Pair the smallest remaining task with the largest remaining task.",
"- Repeat from the outside in. This classic greedy pairing minimizes the worst (maximum) pair sum.",
"",
"---",
"### How the Code Works:",
"1. Index Buckets for Duplicates:",
" - getTaskDurationsToIndices(tasks) builds a dictionary from duration → list of all indices where it occurs.",
" - This lets us sort by duration but still recover the original indices (required by many interview specs).",
"",
"2. Sort Durations Once:",
" - sortedTasks = sorted(tasks) gives the ascending order of durations.",
"",
"3. Two-Pointer Style Pairing:",
" - For idx in 0..k-1:",
" - Take task1Duration = sortedTasks[idx] (current smallest) and pop one original index for it.",
" - Take task2Duration = sortedTasks[len(tasks)-1-idx] (current largest) and pop one original index for it.",
" - Append the pair [task1Index, task2Index] to the result.",
"",
"---",
"### Why Popping from Buckets?",
"- Multiple tasks can have the same duration.",
"- Storing lists of indices for each duration ensures we return valid, unique original positions even with duplicates.",
"",
"---",
"### Complexity:",
"Time Complexity: O(n log n) due to sorting, where n = 2k.",
"Space Complexity: O(n) for the duration→indices buckets and the output.",
"",
"---",
"### Takeaways:",
"- Greedy pairing (smallest with largest) is optimal for minimizing the maximum pair sum.",
"- Mapping durations to original indices cleanly handles duplicates and preserves required outputs."
],
"date": "2025-11-02"
},

 {
"title": "Finding a Valid Starting City on a Circular Route (Gas Station Problem)",
"content": [
"This Python solution finds a city index from which you can start with an empty tank, collect fuel along the way, and complete the entire circular route without running out of gas. Here, mpg stands for miles per gallon."
],
"images": ["algorithms_pics/valid_starting_city.png"],
"description": [
"### Problem Statement:",
"- You have n cities in a circle.",
"- distances[i] = miles from city i to city (i + 1) % n.",
"- fuel[i] = gallons available at city i.",
"- mpg = miles per gallon (fuel efficiency).",
"- Goal: return an index s such that starting at city s (with 0 fuel), refueling at each city, you can travel the full loop without going negative.",
"",
"---",
"### How the Algorithm Works (Single Pass):",
"1. Running balance of miles:",
" - As you sweep cities 1..n-1, maintain milesRemaining.",
" - At each step, add fuel[cityIdx - 1] * mpg and subtract distances[cityIdx - 1].",
"",
"2. Track the lowest dip:",
" - Keep milesRemainingAtStartingCityCandidate, the minimum balance seen so far.",
" - When the current balance becomes a new minimum, set the next city as the new starting candidate.",
"",
"3. Return the candidate:",
" - After one pass, indexOfStartingCityCandidate is a valid start: it comes right after the lowest cumulative dip, so the remaining trip never goes negative.",
"",
"---",
"### Why This Is a Greedy Algorithm:",
"- Local choice: Whenever the running balance hits a new low, we greedily abandon all earlier starts and move the start to the next city. We don’t look ahead or backtrack.",
"- Dominance argument: If starting at city a fails before city b, then any start between a and b also fails by at least as much before reaching b (they all inherit the same deficit). Thus once a deficit occurs, the best available local fix is to restart after the worst dip—no earlier start can beat that choice.",
"- Optimal substructure: The remaining suffix after the minimum prefix-sum point is the subproblem with the highest possible buffer; solving it greedily yields a global solution.",
"",
"---",
"### Code Highlights:",
"- milesRemaining accumulates the net miles gain/loss fuel[i]*mpg - distances[i].",
"- On each new minimum, update indexOfStartingCityCandidate = cityIdx.",
"- One linear scan; no extra arrays or wrap-around loops.",
"",
"---",
"### Complexity:",
"Time: O(n) — single pass over the cities. \nSpace: O(1) — only a handful of variables.",
"",
"---",
"### Edge Note:",
"- A solution exists iff the total fuel (in miles) across all cities is at least the total distance. If not, no starting city can complete the loop."
],
"date": "2025-11-03"
}

]
